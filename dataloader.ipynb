{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEjeSnp3IB-w","executionInfo":{"status":"ok","timestamp":1670272606432,"user_tz":360,"elapsed":23669,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"4457ea6b-0519-435b-a3c7-18f60b46a24d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace\")\n","import sys\n","sys.path.append('.')\n","%autosave 60"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"AtGt4wE_IHMs","executionInfo":{"status":"ok","timestamp":1670272607435,"user_tz":360,"elapsed":1010,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"353a3806-d80a-481f-dcf6-cbb01ffacf94"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["IPython.notebook.set_autosave_interval(60000)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Autosaving every 60 seconds\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YlJ6z8hBo9AM","executionInfo":{"status":"ok","timestamp":1670213269826,"user_tz":360,"elapsed":357,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"a9764051-06a2-4b36-dfc3-b930b5444e1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace\n"]}]},{"cell_type":"code","source":["!pip install trimesh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYLcgkq6fJg4","executionInfo":{"status":"ok","timestamp":1670272638929,"user_tz":360,"elapsed":4656,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"9c1225db-1fcf-4d57-d59c-8edde8d172d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting trimesh\n","  Downloading trimesh-3.17.1-py3-none-any.whl (669 kB)\n","\u001b[K     |████████████████████████████████| 669 kB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from trimesh) (1.21.6)\n","Installing collected packages: trimesh\n","Successfully installed trimesh-3.17.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","pixel_mean = np.array([123.675, 116.280, 103.530]).reshape((1, 1, 3))\n","pixel_std = np.array([58.395, 57.120, 57.375]).reshape((1, 1, 3))\n","max_depth_range = 2.1\n","n_planes_for_train = 5\n","n_planes_for_val = 20\n","n_bins_for_plane_hrchy_sampling = 20\n","mesh_data_root = 'demodata'\n","data_h = 512\n","data_w = 512\n","crop_expand_ratio = 0.1\n","binvoxPathPrefix = 'demodata'\n","use_adaptive_sampling = False\n","bin_sample_replace = False\n","depth_range_expand_ratio = 0.1\n","use_masked_out_img = False\n","extra_mesh_data_root = None"],"metadata":{"id":"OECufHz0d2dX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from dataloader import CustomImageDataset\n","#path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","#dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","#\n","#for i in range(1):\n","#  print(dataset[i][0].shape, dataset[i][1].shape)"],"metadata":{"id":"kKA68EhOde5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import gc\n","import glob\n","import multiprocessing\n","import trimesh\n","import traceback\n","import skimage\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# import meshplot as mp\n","from tqdm import tqdm\n","from PIL import Image\n","from scipy.ndimage.morphology import binary_erosion, distance_transform_edt\n","from scipy.ndimage import maximum_filter, gaussian_filter\n","from scipy import ndimage\n","from skimage import filters, transform  \n","import binvox_rw\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","class MeshObj:\n","    def __init__(self,fn, max_depth_range=None, fnroot=\"\", binvoxPathPrefix='', extra_fnroot=None, pure_infer=False):\n","        self.float_type = np.float32\n","        arr = fn.split('/')\n","        self.csName = arr[-4]\n","        self.objID = int(arr[-3].split('_')[0])\n","        self.frameID = arr[-2].split('_')[0]\n","        self.IMAGE_SIZE = (800, 1280)\n","        self.fnroot = fnroot\n","        self.fn = self.fnroot + fn[1:]\n","\n","        self.extra_fnroot = extra_fnroot\n","        if self.extra_fnroot is not None:\n","            self.extra_fn = self.extra_fnroot + fn[1:]\n","        else:\n","            self.extra_fn = None\n","\n","        self.imfn = self.fnroot + '/{}/images/{}.bmp'.format(self.csName,self.frameID)\n","        if self.extra_fnroot is not None:\n","            self.depthfn = self.extra_fnroot + '/{}/depth/{}.npy'.format(self.csName,self.frameID)\n","            self.visfn = self.extra_fnroot + '/{}/visible/{}.npy'.format(self.csName,self.frameID)\n","        else:\n","            self.depthfn = self.fnroot + '/{}/depth/{}.npy'.format(self.csName,self.frameID)\n","            self.visfn = self.fnroot + '/{}/visible/{}.npy'.format(self.csName,self.frameID)\n","        \n","        self.pure_infer = pure_infer\n","        if not self.pure_infer:\n","            self.binvox = binvoxPathPrefix + self.GetObjFN()[1:-8] + 'voxel.binvox2'\n","            if not os.path.exists(self.binvox):\n","                self.binvox = self.binvox[:-1]\n","            if not os.path.exists(self.binvox):\n","                self.binvox = os.path.join(os.path.dirname(self.binvox), \"voxel_256.binvox2\")\n","            assert os.path.exists(self.binvox), f\"{self.binvox}\"\n","\n","        self.LoadRMatrices()\n","\n","        self.avoid_nan_eps = 1e-8\n","    \n","    def compute_visible_mesh_depth_range(self):\n","        xx, yy, xxi, yyi, select, obj = self.ProjectObjToImage()\n","        vis_verts = np.array(obj.vertices)[select, :]\n","        min_coords = np.min(vis_verts, axis=0)\n","        max_coords = np.max(vis_verts, axis=0)\n","        depth_range = max_coords[2] - min_coords[2]\n","        return depth_range, max_coords[2]\n","        \n","    def LoadRMatrices(self):\n","        if self.extra_fn is not None:\n","            rmatrices_file = sorted(glob.glob(self.extra_fn[:-8] + 'draw_*/rage_matrices_bin.csv'))[0]\n","        else:\n","            rmatrices_file = sorted(glob.glob(self.fn[:-8] + 'draw_*/rage_matrices_bin.csv'))[0]\n","    \n","        rage_matrices = np.fromfile(rmatrices_file,dtype=np.float32).astype(self.float_type)\n","        rage_matrices = rage_matrices.reshape((4,4,4))\n","        self.VP = np.dot(np.linalg.inv(rage_matrices[0,:,:]),rage_matrices[2,:,:])\n","        self.VP_inverse = np.linalg.inv(self.VP) # multiply this matrix to convert from NDC to world coordinate\n","        self.P = np.dot(np.linalg.inv(rage_matrices[1,:,:]),rage_matrices[2,:,:])\n","        self.P_inverse = np.linalg.inv(self.P) # multiply this matrix to convert from NDC to camera coordinate\n","    def GetObjFN(self):\n","        return \".\" + self.fn[len(self.fnroot):]\n","    def GetMesh(self):\n","        return trimesh.load(self.fn)\n","    def LoadBinVox(self):\n","        with open(self.binvox, 'rb') as f:\n","            return binvox_rw.read_as_3d_array(f)\n","    def ndcs_to_pixels(self, x, y):\n","        s_y, s_x = self.IMAGE_SIZE\n","        s_x -= 1\n","        s_y -= 1\n","        xx = self.float_type(x + 1) * self.float_type(s_x / 2)\n","        yy = self.float_type(1 - y) * self.float_type(s_y / 2)\n","        return xx, yy\n","    def pixels_to_ndcs(self, xx, yy):\n","        s_y, s_x = self.IMAGE_SIZE\n","        s_x -= 1  # so 1 is being mapped into (n-1)th pixel\n","        s_y -= 1  # so 1 is being mapped into (n-1)th pixel\n","        x = self.float_type(2 / s_x) * self.float_type(xx) - 1\n","        y = self.float_type(-2 / s_y) * self.float_type(yy) + 1\n","        return x, y\n","    def ProjectObjToImage(self):\n","        obj = trimesh.load(self.fn)\n","        ndcpts = np.concatenate([obj.vertices, np.ones((obj.vertices.shape[0],1))],axis=1) @ self.P\n","        ndcpts = ndcpts[:,0:2]/ndcpts[:,-1:]\n","        xx, yy = self.ndcs_to_pixels(ndcpts[:,0], ndcpts[:,1])\n","        xxi = np.rint(xx).astype(int)\n","        yyi = np.rint(yy).astype(int)\n","        select = np.logical_and(np.logical_and(xxi>0, xxi<self.IMAGE_SIZE[1]), np.logical_and(yyi>0, yyi<self.IMAGE_SIZE[0]))\n","        return xx, yy, xxi, yyi, select, obj\n","    \n","    def __repr__(self):\n","        return \"MeshObj(\\n  {}\\n  {};  {};  {})\".format(self.fn,self.csName,self.objID,self.frameID)\n","    def __str__(self):\n","        return self.__repr__()\n","    \n","    def getOPlanes(self,numDepth,given_depth_range=None,depth_range_expand_ratio=0.1,pure_infer=False):\n","            im = np.array(Image.open(self.imfn))\n","            depth = np.load(self.depthfn).astype(self.float_type)/6.0 - 4e-5\n","            vis_orig = np.load(self.visfn)==self.objID\n","\n","            obj = self.GetMesh()\n","\n","            py_orig, px_orig = np.nonzero(vis_orig)\n","            px = px_orig \n","            py = py_orig \n","            ndcx, ndcy = self.pixels_to_ndcs(px, py)\n","            ndcz = depth[py, px]\n","            rgb = im[py, px]\n","\n","            ndc_coord = np.stack([ndcx, ndcy, ndcz, np.ones_like(ndcz)], axis=1) # NDC\n","            camera_coord = ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","            camera_coord = camera_coord[:,0:3]/camera_coord[:,-1:] # divide, [#pixels, 3]\n","\n","            # mp.plot(camera_coord, c=rgb.astype(np.double)/255, shading={\"point_size\": 0.03})\n","\n","            if given_depth_range is not None:\n","                cur_depth_range = given_depth_range\n","                if pure_infer:\n","                    # NOTE: during inference, we heavily rely on mask to give correct closest_depth value.\n","                    # Therefore, we need to be somehow conservative. \n","                    vis_for_max_Z = binary_erosion(vis_orig, np.ones((10, 10)))\n","                    tmp_py_orig, tmp_px_orig = np.nonzero(vis_for_max_Z)\n","                    tmp_px = tmp_px_orig \n","                    tmp_py = tmp_py_orig \n","                    tmp_ndcx, tmp_ndcy = self.pixels_to_ndcs(tmp_px, tmp_py)\n","                    tmp_ndcz = depth[tmp_py, tmp_px]\n","\n","                    tmp_ndc_coord = np.stack([tmp_ndcx, tmp_ndcy, tmp_ndcz, np.ones_like(tmp_ndcz)], axis=1) # NDC\n","                    tmp_camera_coord = tmp_ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","                    tmp_camera_coord = tmp_camera_coord[:,0:3] / tmp_camera_coord[:,-1:] # divide, [#pixels, 3]\n","                    cur_closest_vis_depth = np.max(tmp_camera_coord[:, 2])\n","                else:\n","                    raise NotImplementedError\n","            else:\n","                cur_depth_range, cur_closest_vis_depth = self.compute_visible_mesh_depth_range()\n","                cur_depth_range = cur_depth_range * (1 + depth_range_expand_ratio)\n","\n","\n","            maxZ = np.max(camera_coord[:, 2])    # neg-Z is for forward. Therefore, maxZ is the closest depth.\n","            zVals = [random.uniform(maxZ - cur_depth_range,maxZ) for _ in range(numDepth)]\n","            aug = np.reshape(zVals, (1, numDepth)).astype(self.float_type)\n","\n","            with open(self.binvox, 'rb') as f:\n","                m1 = binvox_rw.read_as_3d_array(f)\n","\n","            coords = np.tile(camera_coord[...,np.newaxis], (1, 1, numDepth))  # [#pixels, 3, num_depth]\n","\n","            # Path along the ray\n","            coords = coords * aug[:, np.newaxis, :] / (camera_coord[:, 2:, np.newaxis] + self.avoid_nan_eps) # [#p, 3, #d], aug: [1, 1, #d], [#p, 1, 1], [#p, 1, #d]\n","\n","            coords = np.swapaxes(coords,1,2)      # [#pixels, num_depth, 3]\n","            coords = np.reshape(coords, (-1,3))   # [#pixels x num_depth, 3]\n","            # ii = np.tile(np.arange(numDepth), (coords.shape[0]//numDepth,))  # [num_depth x #pixels, ]\n","\n","            grid_coords = np.round((coords - m1.translate) / m1.scale * m1.dims - 0.5).astype(int)\n","            label = np.zeros((coords.shape[0],),dtype=bool)\n","            select = np.logical_and(np.all(grid_coords>=0,axis=1), np.all(grid_coords<m1.dims,axis=1))\n","            label[select] = m1.data[grid_coords[select,0], grid_coords[select,1], grid_coords[select,2]]\n","\n","            gt_im = np.zeros(vis_orig.shape + (numDepth,), dtype=bool)   # [new_h, new_w, num_depth]\n","            trueOPlanes = torch.zeros(numDepth,1,800,1280)\n","            for ii in range(numDepth):\n","                gt_im[py_orig, px_orig, ii] = label[ii::numDepth]\n","                trueOPlanes[ii,0] = torch.tensor([1 if x else 0 for x in gt_im[:, :, ii].reshape(800*1280)]).reshape(800,1280)\n","#                 plt.imshow(trueOPlanes[ii,0])\n","#                 plt.show()\n","            \n","            return trueOPlanes, zVals\n","\n"],"metadata":{"id":"ZIrwSz-yfGV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DATALOADER\n","import os\n","import numpy as np\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","from skimage import filters\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, csv_file, n_planes_for_train):\n","        # making dataframe \n","        self.df2 = pd.read_csv(csv_file) \n","        headerList = ['image', 'visible', 'depth', 'gt_oplane']\n","        self.df2.to_csv(\"inputs.csv\", header=headerList, index=False)\n","          \n","        df2 = pd.read_csv(\"inputs.csv\")\n","        #self.train = train\n","        #self.shuffle = shuffle\n","        self.n_planes_for_train = n_planes_for_train\n","        self.gt_oplanes = df2.loc[:,\"gt_oplane\"]\n","        self.img_dir = df2.loc[:,\"image\"]\n","        self.depth_dir = df2.loc[:,\"depth\"]\n","        self.visible_dir = df2.loc[:,\"visible\"]\n","        #self.transform = transform\n","        #self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_dir)\n","\n","    def grey_transform(self, np_array):\n","        #print(np_array.shape)\n","        grey_image = cv2.cvtColor(np_array, cv2.COLOR_BGR2GRAY)\n","        return grey_image\n","\n","    def concatenate_channels(self, img1, img2, axis=2):\n","        return np.concatenate((img1,img2), axis=2)\n","      \n","    def calculate_euclidean(self, np_array):\n","        return scipy.ndimage.distance_transform_edt(np_array)\n","\n","    def calculate_edges(self, grey_img):\n","        edges = filters.farid(grey_img)\n","        return edges\n","\n","    def resize_image(self, np_array, x_dim, y_dim):\n","        image_size = (x_dim, y_dim)\n","        image_resized = cv2.resize(np_array, image_size, interpolation=cv2.INTER_LINEAR)\n","        return image_resized\n","\n","    def add_channels(self, np_array):\n","        img_resized = self.resize_image(np_array, 512, 512)\n","\n","        grey_img = self.grey_transform(img_resized)\n","        edges = self.calculate_edges(grey_img)\n","        edges_resized = self.resize_image(edges, 512, 512)\n","        edges_resized = torch.from_numpy(edges_resized).unsqueeze(-1)\n","        edges_resized = edges_resized.cpu().detach().numpy()\n","\n","        euc_dist = self.calculate_euclidean(grey_img)\n","        euc_dist_resized = torch.from_numpy(euc_dist).unsqueeze(-1)\n","        euc_dist_resized = euc_dist_resized.cpu().detach().numpy()\n","\n","        concatenated_input = self.concatenate_channels(img_resized,euc_dist_resized, axis=2)\n","        concatenated_input = self.concatenate_channels(concatenated_input,edges_resized, axis=2)\n","\n","        return concatenated_input\n","\n","    def __getitem__(self, idx):\n","        #self.__len__\n","        #if self.shuffle:\n","        #  idx = random.randint(0,self.__len__)\n","\n","        #if self.train:\n","\n","\n","        image = cv2.imread(self.img_dir[idx])\n","        image = self.add_channels(image)\n","      \n","        label_obj = MeshObj(self.gt_oplanes[idx],max_depth_range,mesh_data_root,binvoxPathPrefix,extra_mesh_data_root)\n","        label, zVals = label_obj.getOPlanes(self.n_planes_for_train)\n","        # depth_range, max_z = label_obj.compute_visible_mesh_depth_range()\n","\n","        #print(label.shape)\n","\n","        depth = np.load(self.depth_dir[idx])\n","        depth = self.resize_image(depth, 512,512)\n","        depth = np.expand_dims(depth, axis=0)\n","        depth = torch.from_numpy(depth)\n","\n","        visible = np.load(self.visible_dir[idx])\n","        #visible = self.resize_image(visible, 512,512)\n","        visible_low_res = self.resize_image(visible, 400, 640)\n","        visible = np.expand_dims(visible, axis=0)\n","        visible = torch.from_numpy(visible)\n","        visible_low_res = np.expand_dims(visible_low_res, axis=0)\n","        visible_low_res = torch.from_numpy(visible_low_res)\n","\n","        image = np.moveaxis(image, -1, 0)\n","        image = torch.from_numpy(image)\n","\n","        # label = np.moveaxis(label, -1, 0)\n","        # label = torch.from_numpy(label)\n","\n","        #print(torch.is_tensor(image), torch.is_tensor(label), torch.is_tensor(depth), torch.is_tensor(visible), torch.is_tensor(visible_low_res))\n","\n","        return image, label, depth, visible, visible_low_res, zVals\n","\n"],"metadata":{"id":"iCjX-G1fIXwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from dataloader import CustomImageDataset\n","#path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","#dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","#\n","#for i in range(1):\n","#  print(dataset[i])"],"metadata":{"id":"bRtZUKsuSPAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''FPN in PyTorch.\n","See the paper \"Feature Pyramid Networks for Object Detection\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import cv2\n","import time\n","from torch.autograd import Variable\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class FPN(nn.Module):\n","    def __init__(self, block, num_blocks):\n","        super(FPN, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3, bias=False) #THE FIRST NUM (3) IS THE NUMBER OF INPUT CHANNELS\n","        self.bn1 = nn.BatchNorm2d(64)\n","\n","        # Bottom-up layers\n","        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","\n","        # Top layer\n","        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n","\n","        # Smooth layers\n","        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","\n","        # Lateral layers\n","        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def _upsample_add(self, x, y):\n","        '''Upsample and add two feature maps.\n","        Args:\n","          x: (Variable) top feature map to be upsampled.\n","          y: (Variable) lateral feature map.\n","        Returns:\n","          (Variable) added feature map.\n","        Note in PyTorch, when input size is odd, the upsampled feature map\n","        with `F.upsample(..., scale_factor=2, mode='nearest')`\n","        maybe not equal to the lateral feature map size.\n","        e.g.\n","        original input size: [N,_,15,15] ->\n","        conv2d feature map size: [N,_,8,8] ->\n","        upsampled feature map size: [N,_,16,16]\n","        So we choose bilinear upsample which supports arbitrary output sizes.\n","        '''\n","        _,_,H,W = y.size()\n","        return F.upsample(x, size=(H,W), mode='bilinear') + y\n","\n","    def forward(self, x):\n","        # Bottom-up\n","        c1 = F.relu(self.bn1(self.conv1(x)))\n","        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n","        c2 = self.layer1(c1)\n","        c3 = self.layer2(c2)\n","        c4 = self.layer3(c3)\n","        c5 = self.layer4(c4)\n","        # Top-down\n","        p5 = self.toplayer(c5)\n","        p4 = self._upsample_add(p5, self.latlayer1(c4))\n","        p3 = self._upsample_add(p4, self.latlayer2(c3))\n","        p2 = self._upsample_add(p3, self.latlayer3(c2))\n","        # Smooth\n","        p4 = self.smooth1(p4)\n","        p3 = self.smooth2(p3)\n","        p2 = self.smooth3(p2)\n","        return p2\n","\n","\n","def FPN101():\n","    # return FPN(Bottleneck, [2,4,23,3])\n","    return FPN(Bottleneck, [2,2,2,2])\n","    \n","class FRGB(nn.Module):\n","    def __init__(self):\n","        super(FRGB, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,128,1)\n","        self.gn3 = nn.GroupNorm(32,128)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.gn3(self.conv3(x))\n","        return x\n","\n","class Fspatial(nn.Module):\n","    def __init__(self):\n","        super(Fspatial, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,1,1)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.conv3(x)\n","        return x\n","    \n","class TwoLayerCNN(nn.Module):\n","    def __init__(self):\n","        super(TwoLayerCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(64,128,1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","    \n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.gn2(self.conv2(x))\n","        return x\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        ####################\n","        num = (2*mask*op_true*op_pred).sum(dim = [2,3])\n","        d1 = (mask*op_true).sum(dim=[2,3])\n","        d2 = (mask*op_pred).sum(dim=[2,3])\n","        s = (num/(d1+d2)).sum()\n","        \n","        ####################\n","        #Should be equivalent to this:\n","#         for i in range(N):\n","#             num = 2*torch.sum(mask*op_true[i]*op_pred[i])\n","#             denom = torch.sum(mask*op_true[i])+torch.sum(mask*op_pred[i])\n","#             s = s + num/denom\n","    \n","        return s/N\n","    \n","\n","    \n","class BCELoss(nn.Module):\n","    def __init__(self):\n","        super(BCELoss,self).__init__()\n","    \n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        epsilon = 10 ** -44\n","        num = mask*((op_true*(torch.log(op_pred + epsilon))) + (1-op_true)*torch.log(1-op_pred+epsilon))                 \n","        s = num.sum()\n","        s = - s/(N*mask.sum()) \n","    \n","        return s\n","        \n","        \n","        \n","\n","class ComboNet(nn.Module):\n","    def __init__(self, batchSize, numPeChannels, learningRate):\n","        super(ComboNet, self).__init__()\n","        self.f_FPN = FPN(Bottleneck, [3,4,6,3])\n","        self.f_RGB = FRGB()\n","        self.f_depth = TwoLayerCNN()\n","        self.f_spatial = Fspatial()\n","        \n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n","        self.lossBCE = BCELoss()\n","        self.lossDICE = DiceLoss()\n","        \n","        #Make denominator tensor for positional encoding (don't want to run duplicate work)\n","        denom = torch.zeros(batchSize,numPeChannels,256,256)\n","        idx = torch.ones(256,256)\n","        for i in range(numPeChannels):\n","            denom[:,i,:,:] = 200**(2*i*idx/numPeChannels)\n","        self.denom = denom\n","        self.numPeChannels = numPeChannels\n","        \n","        \n","    def forward(self,x1,x2,z):\n","        st = time.time()\n","        #RGB feature processing\n","        x1 = self.f_FPN(x1)\n","        x1_lowres = self.f_RGB(x1)\n","        x1 = F.interpolate(x1_lowres,scale_factor=2, mode='bilinear') #Upsampling step: convert 128x128 to 256x256 img\n","        \n","        #Depth feature processing\n","        x2 = self.positionEncoding(x2,z)\n","        x2_lowres = F.interpolate(x2,scale_factor=0.5, mode='bilinear') #Downsampled depth difference image\n","        x2 = self.f_depth(x2)\n","        x2_lowres = self.f_depth(x2_lowres)\n","        \n","        #Combine features and pass them through final CNN\n","        x = torch.cat((x1,x2), 1) #second arg specifies which dimension to concatenate on, we want channel dimension which is 1\n","        x = self.f_spatial(x)\n","        \n","        #Get low res OPlane for loss computation, use inner product (eqn 14 from paper)   \n","        multp  = x1_lowres * x2_lowres \n","        x_lowres = multp.sum(dim = 1, keepdim = True)\n","        \n","        #Normalize both outputs so all values are between 0 and 1\n","        x = x - x.min()\n","        x = x/x.max()\n","        x_lowres = x_lowres - x_lowres.min()\n","        x_lowres = x_lowres/x_lowres.max()\n","        \n","        return x, x_lowres\n","    \n","    def positionEncoding(self, depth, z):\n","        \"\"\"\n","        Computes the positional encoding (as defined by the paper) for a depth\n","        - depth: the input depth image\n","        - z: the distance we wish to evaluate\n","        \"\"\"\n","        depth = F.interpolate(depth,scale_factor=0.5, mode='bilinear')\n","        s = depth.size()\n","        pe = torch.zeros(s[0],self.numPeChannels,s[2],s[3])\n","        num = z-depth\n","        pe[:,0::2,:,:] = torch.sin(50*num/self.denom[:,0::2,:])\n","        pe[:,1::2,:,:] = torch.cos(50*num/self.denom[:,1::2,:])\n","        \n","        return pe\n","    \n","    def step(self,x_RGB,x_depth,mask,z_vals,op_true_highres):\n","        \"\"\"\n","        Iterates over a single training step, ie one image with a set of N values in the range [z_min, z_max]\n","        - x: input batch\n","        - y: expected labels for batch\n","        \"\"\"\n","        self.optimizer.zero_grad() #Reset parameter gradients to 0\n","        #Get the outputs for each values of z\n","        N = len(z_vals)\n","        op_highres = torch.zeros(N,1,800,1280)\n","        op_lowres = torch.zeros(N,1,400,640)\n","        \n","        st = time.time()\n","        for i, z in enumerate(z_vals):\n","            op_highres_i, op_lowres_i = self.forward(x_RGB,x_depth,z)\n","            op_highres[i,:,:,:] = F.interpolate(op_highres_i,(800,1280), mode='bilinear')\n","            op_lowres[i,:,:,:] = F.interpolate(op_lowres_i,(400,640), mode='bilinear')\n","        # print(\"Ft: \",round(time.time()-st,2))\n","            \n","        \n","        #Calculate the loss based on the predicted OPlanes for all z values\n","        lambda_BCE, lambda_DICE = 1,1\n","\n","        # downsample to low resolution\n","        mask_lowres = F.interpolate(mask,scale_factor=0.5, mode='bilinear')\n","        # op_true_highres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","        op_true_lowres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","\n","        # # upscale everything to originial dimension (same as GT O-Plane)\n","        # op_true_highres = F.interpolate(op_true_highres,(800,1280), mode='bilinear')\n","        # op_highres = F.interpolate(op_highres,(800,1280), mode='bilinear')\n","        \n","        #print(self.lossBCE(op_true_highres, op_highres, mask, N), self.lossDICE(op_true_highres, op_highres, mask, N))\n","        #op_highres = torch.where(op_highres == 0, 0.001 + op_highres, op_highres)\n","        #op_highres = torch.where(op_highres == 1, op_highres - 0.001, op_highres)\n","\n","        #print(op_true_highres, op_highres, mask)\n","        #print(self.lossBCE(op_true_highres, op_highres, mask, N), self.lossDICE(op_true_highres, op_highres, mask, N))\n","\n","        loss_highres = lambda_BCE*self.lossBCE(op_true_highres, op_highres, mask, N) + lambda_DICE*self.lossDICE(op_true_highres, op_highres, mask, N)\n","        loss_lowres = lambda_BCE*self.lossBCE(op_true_lowres, op_lowres, mask_lowres, N) + lambda_DICE*self.lossDICE(op_true_lowres, op_lowres, mask_lowres, N)    \n","\n","        #print(loss_highres, loss_lowres)\n","\n","        loss = loss_highres + loss_lowres\n","        st = time.time()\n","        #loss.requires_grad = True\n","        loss.backward()\n","        # print(\"Bt: \",round(time.time()-st,2))\n","        \n","        self.optimizer.step()\n","\n","        return loss.detach().cpu().numpy()"],"metadata":{"id":"_rZjKbWHwLFg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.sampler import SubsetRandomSampler\n","import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","import random\n","import math\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","#from modules import ComboNet\n","\n","path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","batch_size = 1\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n","                                           sampler=train_sampler, persistent_workers=True, num_workers=4)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler,persistent_workers=True, num_workers=4)\n","\n","N = 10\n","net = ComboNet(1,64,0.001)\n","# Usage Example:\n","num_epochs = 3\n","totalLoss = 0\n","totalImagesProcessed = 0\n","\n","#File to write losses to\n","f = open(\"loss_training.txt\", \"w\")\n","f.write(\"Losses after each iteration during training:\")\n","f.close()\n","for epoch in range(1,num_epochs):\n","    print(\"Processing epoch \",epoch)\n","    f = open(\"loss_training.txt\", \"w\")\n","    f.write(\"\\n\\nEpoch \"+str(epoch))\n","    f.close()\n","\n","    # Train:   \n","    for i, data in enumerate(train_loader):\n","        \n","        st = time.time()\n","        \n","        op_truth = data[1].to(dtype=torch.float32) \n","        mask = data[3].to(dtype=torch.float32)\n","        mask_lowres = data[4].to(dtype=torch.float32)\n","        z_vals = data[5]\n","        \n","        #Get RGB and Depth images from data\n","        ipt1 = data[0].to(dtype=torch.float32) # image\n","        ipt2 = data[2].to(dtype=torch.float32) # depth\n","\n","        if epoch == 1:\n","          net = ComboNet(1,64,0.0001)\n","          net.load_state_dict(torch.load(\"comboNet.pth\"))\n","\n","        if epoch == 2:\n","          net = ComboNet(1,64,0.0001)\n","          net.load_state_dict(torch.load(\"comboNet.pth\"))\n","\n","        l = net.step(ipt1,ipt2,mask,z_vals,op_truth[0])\n","\n","        #Tally the loss\n","        if not math.isnan(l):\n","          totalLoss += l\n","          totalImagesProcessed += 1\n","          avgLoss = round(totalLoss/totalImagesProcessed,4)\n","        print(\"image \",i,int(time.time()-st), avgLoss,math.isnan(l))\n","\n","        f = open(\"loss_training.txt\", \"a\")\n","        f.write(\"\\n\"+str(avgLoss))\n","        f.close()\n","\n","        #Save model\n","        if i%10 == 0:\n","          torch.save(net.state_dict(),\"comboNet.pth\")\n","        \n","\n","\n"],"metadata":{"id":"QVSVg1IAaPXJ","colab":{"base_uri":"https://localhost:8080/","height":416},"executionInfo":{"status":"error","timestamp":1670258810136,"user_tz":360,"elapsed":66174,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"6691e62f-4fb7-44ae-e245-026745ba4506"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing epoch  1\n","image  0 50 0.7865 False\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d55a1fd40331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"comboNet.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mipt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#Tally the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9f1e6037bd69>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x_RGB, x_depth, mask, z_vals, op_true_highres)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mop_highres_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_lowres_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_RGB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mop_highres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_highres_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mop_lowres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_lowres_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9f1e6037bd69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, z)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m#Combine features and pass them through final CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#second arg specifies which dimension to concatenate on, we want channel dimension which is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_spatial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m#Get low res OPlane for loss computation, use inner product (eqn 14 from paper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9f1e6037bd69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Validation\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","import random\n","import math\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","#from modules import ComboNet\n","\n","path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","batch_size = 1\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","valid_sampler = SubsetRandomSampler(val_indices)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler,persistent_workers=True, num_workers=4)\n","totalLoss = 0\n","totalImagesProcessed = 0\n","N = 10\n","net = ComboNet(1,64,0.001)\n","saved_model = \"comboNet.pth\"\n","net.load_state_dict(torch.load(saved_model))\n","#File to write losses to\n","f = open(\"loss_validation.txt\", \"w\")\n","f.write(\"Losses after each iteration during training:\")\n","f.close()\n","\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    print(\"Processing validation \",epoch)\n","    f = open(\"loss_validation.txt\", \"w\")\n","    f.write(\"\\n\\nEpoch \"+str(epoch))\n","    f.close()\n","\n","    # Train:   \n","    for i, data in enumerate(validation_loader):\n","        \n","        st = time.time()\n","        \n","        op_truth = data[1].to(dtype=torch.float32) \n","        mask = data[3].to(dtype=torch.float32)\n","        mask_lowres = data[4].to(dtype=torch.float32)\n","        z_vals = data[5]\n","        \n","        #Get RGB and Depth images from data\n","        ipt1 = data[0].to(dtype=torch.float32) # image\n","        ipt2 = data[2].to(dtype=torch.float32) # depth\n","\n","        l = net.step(ipt1,ipt2,mask,z_vals,op_truth[0])\n","        print(l)\n","\n","        #Tally the loss\n","        if not math.isnan(l):\n","          totalLoss += l\n","          totalImagesProcessed += 1\n","          avgLoss = round(totalLoss/totalImagesProcessed,4)\n","        print(\"image \",i,int(time.time()-st), avgLoss,math.isnan(l))\n","\n","        f = open(\"loss_validation.txt\", \"a\")\n","        f.write(\"\\n\"+str(avgLoss))\n","        f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"dgpx368WpciK","executionInfo":{"status":"error","timestamp":1670209775770,"user_tz":360,"elapsed":55444,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"d84d30e4-819d-4cf9-f308-ca119ac02d23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing epoch  0\n","1.1799917\n","image  0 40 1.18 False\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-88acb8f36c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mipt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mipt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9f1e6037bd69>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x_RGB, x_depth, mask, z_vals, op_true_highres)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mop_highres_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_lowres_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_RGB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mop_highres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_highres_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mop_lowres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_lowres_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9f1e6037bd69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, z)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m#Combine features and pass them through final CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#second arg specifies which dimension to concatenate on, we want channel dimension which is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_spatial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m#Get low res OPlane for loss computation, use inner product (eqn 14 from paper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9f1e6037bd69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ivaqy-2idy1y","executionInfo":{"status":"ok","timestamp":1670213222498,"user_tz":360,"elapsed":543,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"b754987f-5f5f-44de-d9c2-d0d6ec021dfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv(\"loss_training_printed.txt\",header = None, sep=' ')\n","df.drop(df.columns[[0, 1, 2, 3,5,6,7]],axis=1,inplace=True)\n","#print(df)\n","#df_new.to_csv('sample_text_file_new.txt', header=True, index=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2S6N6OyKdOGG","executionInfo":{"status":"ok","timestamp":1670272898474,"user_tz":360,"elapsed":416,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"4cc7adf3-0268-47f6-972b-a0d184dd04d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["          4\n","0    2.0485\n","1    1.8629\n","2    1.3788\n","3    1.0907\n","4    0.9016\n","..      ...\n","853  0.3951\n","854  0.3953\n","855  0.3948\n","856  0.3946\n","857  0.3944\n","\n","[858 rows x 1 columns]\n"]}]},{"cell_type":"code","source":["df.to_csv('loss_training_new.txt', header=False, index=None)"],"metadata":{"id":"ttt9GcUwg1Lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import torch\n","x = torch.ones(1,256,256)\n","y = torch.zeros(4, 1,256,256)\n","for i in range(4):\n","  y[i,:,:,:] = x\n","\n","print(y.shape)\n","x = torch.unsqueeze(x, dim=0)\n","print(x.dtype)\n","\n","z = F.interpolate(y,(800,1280), mode='bilinear')\n","print(z.shape)"],"metadata":{"id":"npWQLo_h5TvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_obj = MeshObj(self.gt_oplanes[idx],max_depth_range,mesh_data_root,binvoxPathPrefix,extra_mesh_data_root)\n","label = label_obj.Get_GroundTruth(n_planes_for_train)"],"metadata":{"id":"Vv-8j_CufC0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pixel_mean = np.array([123.675, 116.280, 103.530]).reshape((1, 1, 3))\n","pixel_std = np.array([58.395, 57.120, 57.375]).reshape((1, 1, 3))\n","max_depth_range = 2.1\n","n_planes_for_train = 5\n","n_planes_for_val = 20\n","n_bins_for_plane_hrchy_sampling = 20\n","mesh_data_root = 'demodata'\n","data_h = 512\n","data_w = 512\n","crop_expand_ratio = 0.1\n","binvoxPathPrefix = 'demodata'\n","use_adaptive_sampling = False\n","bin_sample_replace = False\n","depth_range_expand_ratio = 0.1\n","use_masked_out_img = False\n","extra_mesh_data_root = None"],"metadata":{"id":"3kEmEO_wft0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from dataloader import CustomImageDataset\n","path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","\n","for i in range(1):\n","  print(dataset[i][4].shape)"],"metadata":{"id":"C--7Khd3UR4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainModel(N):\n","    torch.autograd.set_detect_anomaly(False)\n","    \n","    #Instantiate model\n","    net = ComboNet(1,64,0.001)\n","    \n","    #Get data \n","    epochs = 1\n","    data = torch.ones(1,3,512,512)\n","    \n","    #Training loop\n","    st = time.time()\n","    for i in range(epochs):\n","        print(\"Processing epoch \",i)\n","        \n","        for d in data:\n","            \n","            #Generate set of Z's\n","            z_min = 1 #Calculated using min(depth OR mask), ie the point on the object of interest that is closest to the camera\n","            z_range = 2 #For training we need to get this from the the mesh to find the ground truth range of depths\n","            z_vals = z_range*torch.rand(N) + z_min #create 10 random values between z_min and z_max\n","            \n","            op_truth = torch.ones(N,1,512,512)#Q: WHERE TO GET THESE FROM? THE MESH? Dimension is N samples, each with one channel and size HxW\n","            mask = torch.ones(1,1,512,512)\n","            mask_lowres = torch.ones(1,1,128,128)\n","            \n","            \n","            #Get RGB and Depth images from data\n","            ipt1 = torch.ones(1,3,512,512)\n","            ipt2 = torch.ones(1,1,512,512)\n","            net.step(ipt1,ipt2,mask,z_vals,op_truth)\n","            \n","\n","\n","    print(\"Total training time: \",round(time.time()-st,2))\n","    \n","\n","trainModel(2) #N (number of OPlanes we sample per image) is 10 during training"],"metadata":{"id":"9QFvDdqZc-M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","print(path + '/' + 'inputs.csv')"],"metadata":{"id":"aI4up2W_VydN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"vz88vhHITVr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display image and label.\n","import matplotlib.pyplot as plt\n","\n","train_features, train_labels = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0].squeeze()\n","label = train_labels[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"],"metadata":{"id":"h8P_p5S5IdoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas module \n","import pandas as pd \n","    \n","# making dataframe \n","df = pd.read_csv(\"inputs.csv\") \n","headerList = ['image', 'visible', 'depth', 'gt_oplane']\n","df.to_csv(\"inputs.csv\", header=headerList, index=False)\n","   \n","df2 = pd.read_csv(\"inputs.csv\")\n","print(df2['gt_oplane'][0])\n"],"metadata":{"id":"gS7hZvHMMI60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(0.8*len(df2))"],"metadata":{"id":"i0jGiIIiTlUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","training_losses = pd.read_csv(\"loss_training.txt\")\n","print(training_losses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6u6DxabTnX4","executionInfo":{"status":"ok","timestamp":1670272977499,"user_tz":360,"elapsed":150,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"7a760863-a79d-4696-fbee-43f2255685fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     2.0485\n","0    1.8629\n","1    1.3788\n","2    1.0907\n","3    0.9016\n","4    0.7758\n","..      ...\n","852  0.3951\n","853  0.3953\n","854  0.3948\n","855  0.3946\n","856  0.3944\n","\n","[857 rows x 1 columns]\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","x = training_losses.values.tolist()\n","x = np.array(x).reshape(857,)\n","#print(training_losses)\n","#print(x)\n","#y = np.linspace(0,1,10)\n","y = np.arange(857)\n","#print(len(x), y)\n","\n","#x1 = np.linspace(0, 1, 857)\n","#x2 = np.linspace(0, 1, 10)\n","#plt.plot(x1, x)\n","#plt.plot(x2, y)\n","plt.show()"],"metadata":{"id":"ZE_FRoBfAM_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title(\"Training loss per image sample\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"image sample\")\n","plt.plot(x)"],"metadata":{"id":"FKx-Ip0GCOP1","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"ok","timestamp":1670274805678,"user_tz":360,"elapsed":470,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"624d2de8-5d48-4e1e-d37a-7c0ca099e139"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f3504c534c0>]"]},"metadata":{},"execution_count":42},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["validation_losses = pd.read_csv(\"loss_validation.txt\")\n","x2 = validation_losses.values.tolist()\n","x2 = np.array(x2).reshape(71,)\n","y2 = np.arange(71)\n","plt.title(\"Validation loss per image sample\")\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"image sample\")\n","plt.plot(x2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"26BeyrzvED_f","executionInfo":{"status":"ok","timestamp":1670274960490,"user_tz":360,"elapsed":982,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"b837a923-5e66-46f4-a16d-373184c8950c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f35040d6670>]"]},"metadata":{},"execution_count":46},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[],"metadata":{"id":"Zge6N7fxJAZ9"},"execution_count":null,"outputs":[]}]}