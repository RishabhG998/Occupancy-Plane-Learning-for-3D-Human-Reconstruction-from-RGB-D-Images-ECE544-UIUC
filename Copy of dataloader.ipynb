{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ETNleL0fgTd2uKOBsWoY62pxUlTkHLHb","timestamp":1670106656043}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEjeSnp3IB-w","executionInfo":{"status":"ok","timestamp":1670107520656,"user_tz":360,"elapsed":1626,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"b47b60de-02fd-4c63-9c6b-d7394b36a806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/ECE 544 Project/Viktor's Workspace\")\n","import sys\n","sys.path.append('.')\n","%autosave 60"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"AtGt4wE_IHMs","executionInfo":{"status":"ok","timestamp":1670107522908,"user_tz":360,"elapsed":216,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"268cdf2e-2b28-4b0a-dce4-2095dbfd2e18"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"application/javascript":["IPython.notebook.set_autosave_interval(60000)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Autosaving every 60 seconds\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YlJ6z8hBo9AM","executionInfo":{"status":"ok","timestamp":1670107489057,"user_tz":360,"elapsed":218,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"8d25004b-087b-46a7-f474-a5152ed982c2"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["'Copy of dataloader.ipynb'   FPN.py   OPlanesModel.ipynb\n"]}]},{"cell_type":"code","source":["!pip install trimesh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"eYLcgkq6fJg4","executionInfo":{"status":"ok","timestamp":1670107500862,"user_tz":360,"elapsed":7341,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"bbe545e4-8709-4134-cce9-a96fa4313c9c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting trimesh\n","  Using cached trimesh-3.17.1-py3-none-any.whl (669 kB)\n","Collecting numpy\n","  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[K     |████████████████████████████████| 15.7 MB 12.6 MB/s \n","\u001b[?25hInstalling collected packages: numpy, trimesh\n","Successfully installed numpy-1.21.6 trimesh-3.17.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","trimesh"]}}},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","pixel_mean = np.array([123.675, 116.280, 103.530]).reshape((1, 1, 3))\n","pixel_std = np.array([58.395, 57.120, 57.375]).reshape((1, 1, 3))\n","max_depth_range = 2.1\n","n_planes_for_train = 5\n","n_planes_for_val = 20\n","n_bins_for_plane_hrchy_sampling = 20\n","mesh_data_root = 'demodata'\n","data_h = 512\n","data_w = 512\n","crop_expand_ratio = 0.1\n","binvoxPathPrefix = 'demodata'\n","use_adaptive_sampling = False\n","bin_sample_replace = False\n","depth_range_expand_ratio = 0.1\n","use_masked_out_img = False\n","extra_mesh_data_root = None"],"metadata":{"id":"OECufHz0d2dX","executionInfo":{"status":"ok","timestamp":1670107528355,"user_tz":360,"elapsed":198,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip3 install binvox"],"metadata":{"id":"kKA68EhOde5t","executionInfo":{"status":"ok","timestamp":1670107623770,"user_tz":360,"elapsed":5116,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"add07eae-0a70-4eef-e430-8bee3f1e061f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting binvox\n","  Using cached binvox-0.1.5.tar.gz (4.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from binvox) (1.21.6)\n","Building wheels for collected packages: binvox\n","  Building wheel for binvox (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for binvox: filename=binvox-0.1.5-py3-none-any.whl size=5169 sha256=9e498633814776c52fe9a15834c136a7cc4c39ca1ccc103a8c72e3243fb04144\n","  Stored in directory: /root/.cache/pip/wheels/06/64/ac/8fbdae9eb134b24f83c43202571c04402c478283d316dfbb97\n","Successfully built binvox\n","Installing collected packages: binvox\n","Successfully installed binvox-0.1.5\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/dimatura/binvox-rw-py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsthgTMDLIu5","executionInfo":{"status":"ok","timestamp":1670107710136,"user_tz":360,"elapsed":1989,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"19ab007d-5f66-44b2-9709-37e6b36fa971"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'binvox-rw-py'...\n","remote: Enumerating objects: 66, done.\u001b[K\n","remote: Total 66 (delta 0), reused 0 (delta 0), pack-reused 66\u001b[K\n","Unpacking objects: 100% (66/66), done.\n"]}]},{"cell_type":"code","source":["%cd binvox-rw-py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5Z2yy_aLYoo","executionInfo":{"status":"ok","timestamp":1670107787778,"user_tz":360,"elapsed":5,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"c42f30a8-4d6b-49f9-921f-293c23662268"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1O2_o-dWDwNOdSb8R9Vbw2wd_nu3HwhKC/ECE 544 Project/Viktor's Workspace/binvox-rw-py\n"]}]},{"cell_type":"code","source":["!mv binvox_rw.py .."],"metadata":{"id":"r_rNjpZtLoUi","executionInfo":{"status":"ok","timestamp":1670107829513,"user_tz":360,"elapsed":4,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import gc\n","import glob\n","import multiprocessing\n","import trimesh\n","import traceback\n","import skimage\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# import meshplot as mp\n","from tqdm import tqdm\n","from PIL import Image\n","from scipy.ndimage.morphology import binary_erosion, distance_transform_edt\n","from scipy.ndimage import maximum_filter, gaussian_filter\n","from scipy import ndimage\n","from skimage import filters, transform  \n","import binvox_rw\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","class MeshObj:\n","    def __init__(self,fn, max_depth_range=None, fnroot=\"\", binvoxPathPrefix='', extra_fnroot=None, pure_infer=False):\n","        self.float_type = np.float32\n","        arr = fn.split('/')\n","        self.csName = arr[-4]\n","        self.objID = int(arr[-3].split('_')[0])\n","        self.frameID = arr[-2].split('_')[0]\n","        self.IMAGE_SIZE = (800, 1280)\n","        self.fnroot = fnroot\n","        self.fn = self.fnroot + fn[1:]\n","\n","        self.extra_fnroot = extra_fnroot\n","        if self.extra_fnroot is not None:\n","            self.extra_fn = self.extra_fnroot + fn[1:]\n","        else:\n","            self.extra_fn = None\n","\n","        self.imfn = self.fnroot + '/{}/images/{}.bmp'.format(self.csName,self.frameID)\n","        if self.extra_fnroot is not None:\n","            self.depthfn = self.extra_fnroot + '/{}/depth/{}.npy'.format(self.csName,self.frameID)\n","            self.visfn = self.extra_fnroot + '/{}/visible/{}.npy'.format(self.csName,self.frameID)\n","        else:\n","            self.depthfn = self.fnroot + '/{}/depth/{}.npy'.format(self.csName,self.frameID)\n","            self.visfn = self.fnroot + '/{}/visible/{}.npy'.format(self.csName,self.frameID)\n","        \n","        self.pure_infer = pure_infer\n","        if not self.pure_infer:\n","            self.binvox = binvoxPathPrefix + self.GetObjFN()[1:-8] + 'voxel.binvox2'\n","            if not os.path.exists(self.binvox):\n","                self.binvox = self.binvox[:-1]\n","            if not os.path.exists(self.binvox):\n","                self.binvox = os.path.join(os.path.dirname(self.binvox), \"voxel_256.binvox2\")\n","            assert os.path.exists(self.binvox), f\"{self.binvox}\"\n","\n","        self.LoadRMatrices()\n","\n","        self.avoid_nan_eps = 1e-8\n","    \n","    def compute_visible_mesh_depth_range(self):\n","        xx, yy, xxi, yyi, select, obj = self.ProjectObjToImage()\n","        vis_verts = np.array(obj.vertices)[select, :]\n","        min_coords = np.min(vis_verts, axis=0)\n","        max_coords = np.max(vis_verts, axis=0)\n","        depth_range = max_coords[2] - min_coords[2]\n","        return depth_range, max_coords[2]\n","        \n","    def LoadRMatrices(self):\n","        if self.extra_fn is not None:\n","            rmatrices_file = sorted(glob.glob(self.extra_fn[:-8] + 'draw_*/rage_matrices_bin.csv'))[0]\n","        else:\n","            rmatrices_file = sorted(glob.glob(self.fn[:-8] + 'draw_*/rage_matrices_bin.csv'))[0]\n","    \n","        rage_matrices = np.fromfile(rmatrices_file,dtype=np.float32).astype(self.float_type)\n","        rage_matrices = rage_matrices.reshape((4,4,4))\n","        self.VP = np.dot(np.linalg.inv(rage_matrices[0,:,:]),rage_matrices[2,:,:])\n","        self.VP_inverse = np.linalg.inv(self.VP) # multiply this matrix to convert from NDC to world coordinate\n","        self.P = np.dot(np.linalg.inv(rage_matrices[1,:,:]),rage_matrices[2,:,:])\n","        self.P_inverse = np.linalg.inv(self.P) # multiply this matrix to convert from NDC to camera coordinate\n","    def GetObjFN(self):\n","        return \".\" + self.fn[len(self.fnroot):]\n","    def GetMesh(self):\n","        return trimesh.load(self.fn)\n","    def LoadBinVox(self):\n","        with open(self.binvox, 'rb') as f:\n","            return binvox_rw.read_as_3d_array(f)\n","    def ndcs_to_pixels(self, x, y):\n","        s_y, s_x = self.IMAGE_SIZE\n","        s_x -= 1\n","        s_y -= 1\n","        xx = self.float_type(x + 1) * self.float_type(s_x / 2)\n","        yy = self.float_type(1 - y) * self.float_type(s_y / 2)\n","        return xx, yy\n","    def pixels_to_ndcs(self, xx, yy):\n","        s_y, s_x = self.IMAGE_SIZE\n","        s_x -= 1  # so 1 is being mapped into (n-1)th pixel\n","        s_y -= 1  # so 1 is being mapped into (n-1)th pixel\n","        x = self.float_type(2 / s_x) * self.float_type(xx) - 1\n","        y = self.float_type(-2 / s_y) * self.float_type(yy) + 1\n","        return x, y\n","    def ProjectObjToImage(self):\n","        obj = trimesh.load(self.fn)\n","        ndcpts = np.concatenate([obj.vertices, np.ones((obj.vertices.shape[0],1))],axis=1) @ self.P\n","        ndcpts = ndcpts[:,0:2]/ndcpts[:,-1:]\n","        xx, yy = self.ndcs_to_pixels(ndcpts[:,0], ndcpts[:,1])\n","        xxi = np.rint(xx).astype(int)\n","        yyi = np.rint(yy).astype(int)\n","        select = np.logical_and(np.logical_and(xxi>0, xxi<self.IMAGE_SIZE[1]), np.logical_and(yyi>0, yyi<self.IMAGE_SIZE[0]))\n","        return xx, yy, xxi, yyi, select, obj\n","    \n","    def __repr__(self):\n","        return \"MeshObj(\\n  {}\\n  {};  {};  {})\".format(self.fn,self.csName,self.objID,self.frameID)\n","    def __str__(self):\n","        return self.__repr__()\n","    \n","    # def PlotData(self,numDepth,given_depth_range=None,depth_range_expand_ratio=0.1,pure_infer=False):\n","    #     im = np.array(Image.open(self.imfn))\n","    #     plt.imshow(im)\n","    #     plt.show()\n","    #     depth = np.load(self.depthfn).astype(self.float_type)/6.0 - 4e-5\n","    #     plt.imshow(depth)\n","    #     plt.colorbar()\n","    #     plt.show()\n","    #     vis_orig = np.load(self.visfn)==self.objID\n","    #     plt.imshow(vis_orig)\n","    #     plt.show()\n","\n","    #     obj = self.GetMesh()\n","    #     p = mp.plot(obj.vertices, shading={\"point_color\": \"blue\", \"point_size\": 0.03}, return_plot=True)\n","    #     p.add_edges(obj.vertices, obj.faces, shading={\"line_color\": \"black\"})\n","\n","    #     py_orig, px_orig = np.nonzero(vis_orig)\n","    #     px = px_orig \n","    #     py = py_orig \n","    #     ndcx, ndcy = self.pixels_to_ndcs(px, py)\n","    #     ndcz = depth[py, px]\n","    #     rgb = im[py, px]\n","\n","    #     ndc_coord = np.stack([ndcx, ndcy, ndcz, np.ones_like(ndcz)], axis=1) # NDC\n","    #     camera_coord = ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","    #     camera_coord = camera_coord[:,0:3]/camera_coord[:,-1:] # divide, [#pixels, 3]\n","\n","    #     mp.plot(camera_coord, c=rgb.astype(np.double)/255, shading={\"point_size\": 0.03})\n","\n","    #     if given_depth_range is not None:\n","    #         cur_depth_range = given_depth_range\n","    #         if pure_infer:\n","    #             # NOTE: during inference, we heavily rely on mask to give correct closest_depth value.\n","    #             # Therefore, we need to be somehow conservative. \n","    #             vis_for_max_Z = binary_erosion(vis_orig, np.ones((10, 10)))\n","    #             tmp_py_orig, tmp_px_orig = np.nonzero(vis_for_max_Z)\n","    #             tmp_px = tmp_px_orig \n","    #             tmp_py = tmp_py_orig \n","    #             tmp_ndcx, tmp_ndcy = self.pixels_to_ndcs(tmp_px, tmp_py)\n","    #             tmp_ndcz = depth[tmp_py, tmp_px]\n","\n","    #             tmp_ndc_coord = np.stack([tmp_ndcx, tmp_ndcy, tmp_ndcz, np.ones_like(tmp_ndcz)], axis=1) # NDC\n","    #             tmp_camera_coord = tmp_ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","    #             tmp_camera_coord = tmp_camera_coord[:,0:3] / tmp_camera_coord[:,-1:] # divide, [#pixels, 3]\n","    #             cur_closest_vis_depth = np.max(tmp_camera_coord[:, 2])\n","    #         else:\n","    #             raise NotImplementedError\n","    #     else:\n","    #         cur_depth_range, cur_closest_vis_depth = self.compute_visible_mesh_depth_range()\n","    #         cur_depth_range = cur_depth_range * (1 + depth_range_expand_ratio)\n","\n","\n","    #     maxZ = np.max(camera_coord[:, 2])    # neg-Z is for forward. Therefore, maxZ is the closest depth.\n","    #     aug = np.reshape(np.linspace(maxZ, maxZ - cur_depth_range, numDepth), (1, numDepth)).astype(self.float_type)\n","\n","    #     with open(self.binvox, 'rb') as f:\n","    #         m1 = binvox_rw.read_as_3d_array(f)\n","\n","    #     coords = np.tile(camera_coord[...,np.newaxis], (1, 1, numDepth))  # [#pixels, 3, num_depth]\n","\n","    #     # Path along the ray\n","    #     coords = coords * aug[:, np.newaxis, :] / (camera_coord[:, 2:, np.newaxis] + self.avoid_nan_eps) # [#p, 3, #d], aug: [1, 1, #d], [#p, 1, 1], [#p, 1, #d]\n","\n","    #     coords = np.swapaxes(coords,1,2)      # [#pixels, num_depth, 3]\n","    #     coords = np.reshape(coords, (-1,3))   # [#pixels x num_depth, 3]\n","    #     # ii = np.tile(np.arange(numDepth), (coords.shape[0]//numDepth,))  # [num_depth x #pixels, ]\n","\n","    #     grid_coords = np.round((coords - m1.translate) / m1.scale * m1.dims - 0.5).astype(int)\n","    #     label = np.zeros((coords.shape[0],),dtype=bool)\n","    #     select = np.logical_and(np.all(grid_coords>=0,axis=1), np.all(grid_coords<m1.dims,axis=1))\n","    #     label[select] = m1.data[grid_coords[select,0], grid_coords[select,1], grid_coords[select,2]]\n","\n","    #     p = mp.plot(obj.vertices, shading={\"point_color\": \"blue\", \"point_size\": 0.03}, return_plot=True)\n","    #     p.add_edges(obj.vertices, obj.faces, shading={\"line_color\": \"black\"})\n","    #     p.add_points(coords, c=label, shading={\"point_color\": \"green\", \"point_size\": 0.3})\n","\n","    #     print(\"Given image, depth map, mask, camera information and depth of a plane, predict occupancies like the ones illustrated next:\")\n","\n","    #     gt_im = np.zeros(vis_orig.shape + (numDepth,), dtype=bool)   # [new_h, new_w, num_depth]\n","    #     #print(gt_im[0,:,:])\n","\n","    #     for ii in range(numDepth):\n","    #         gt_im[py_orig, px_orig, ii] = label[ii::numDepth]\n","    #         plt.imshow(gt_im[:,:,ii])\n","    #         plt.show()\n","\n","    def Get_GroundTruth(self,numDepth):\n","        vis_orig = np.load(self.visfn)==self.objID\n","        gt_im = np.zeros(vis_orig.shape + (numDepth,), dtype=bool) \n","        return gt_im\n","    \n","    def getOPlanes(self,numDepth,given_depth_range=None,depth_range_expand_ratio=0.1,pure_infer=False):\n","            im = np.array(Image.open(self.imfn))\n","            depth = np.load(self.depthfn).astype(self.float_type)/6.0 - 4e-5\n","            vis_orig = np.load(self.visfn)==self.objID\n","\n","            obj = self.GetMesh()\n","\n","            py_orig, px_orig = np.nonzero(vis_orig)\n","            px = px_orig \n","            py = py_orig \n","            ndcx, ndcy = self.pixels_to_ndcs(px, py)\n","            ndcz = depth[py, px]\n","            rgb = im[py, px]\n","\n","            ndc_coord = np.stack([ndcx, ndcy, ndcz, np.ones_like(ndcz)], axis=1) # NDC\n","            camera_coord = ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","            camera_coord = camera_coord[:,0:3]/camera_coord[:,-1:] # divide, [#pixels, 3]\n","\n","            if given_depth_range is not None:\n","                cur_depth_range = given_depth_range\n","                if pure_infer:\n","                    # NOTE: during inference, we heavily rely on mask to give correct closest_depth value.\n","                    # Therefore, we need to be somehow conservative. \n","                    vis_for_max_Z = binary_erosion(vis_orig, np.ones((10, 10)))\n","                    tmp_py_orig, tmp_px_orig = np.nonzero(vis_for_max_Z)\n","                    tmp_px = tmp_px_orig \n","                    tmp_py = tmp_py_orig \n","                    tmp_ndcx, tmp_ndcy = self.pixels_to_ndcs(tmp_px, tmp_py)\n","                    tmp_ndcz = depth[tmp_py, tmp_px]\n","\n","                    tmp_ndc_coord = np.stack([tmp_ndcx, tmp_ndcy, tmp_ndcz, np.ones_like(tmp_ndcz)], axis=1) # NDC\n","                    tmp_camera_coord = tmp_ndc_coord @ self.P_inverse # convert to camera coordinate, [#pixels, 3]\n","                    tmp_camera_coord = tmp_camera_coord[:,0:3] / tmp_camera_coord[:,-1:] # divide, [#pixels, 3]\n","                    cur_closest_vis_depth = np.max(tmp_camera_coord[:, 2])\n","                else:\n","                    raise NotImplementedError\n","            else:\n","                cur_depth_range, cur_closest_vis_depth = self.compute_visible_mesh_depth_range()\n","                cur_depth_range = cur_depth_range * (1 + depth_range_expand_ratio)\n","\n","\n","            maxZ = np.max(camera_coord[:, 2])    # neg-Z is for forward. Therefore, maxZ is the closest depth.\n","            zVals = [random.uniform(maxZ - cur_depth_range,maxZ) for _ in range(numDepth)]\n","            aug = np.reshape(zVals, (1, numDepth)).astype(self.float_type)\n","\n","            with open(self.binvox, 'rb') as f:\n","                m1 = binvox_rw.read_as_3d_array(f)\n","\n","            coords = np.tile(camera_coord[...,np.newaxis], (1, 1, numDepth))  # [#pixels, 3, num_depth]\n","\n","            # Path along the ray\n","            coords = coords * aug[:, np.newaxis, :] / (camera_coord[:, 2:, np.newaxis] + self.avoid_nan_eps) # [#p, 3, #d], aug: [1, 1, #d], [#p, 1, 1], [#p, 1, #d]\n","\n","            coords = np.swapaxes(coords,1,2)      # [#pixels, num_depth, 3]\n","            coords = np.reshape(coords, (-1,3))   # [#pixels x num_depth, 3]\n","            # ii = np.tile(np.arange(numDepth), (coords.shape[0]//numDepth,))  # [num_depth x #pixels, ]\n","\n","            grid_coords = np.round((coords - m1.translate) / m1.scale * m1.dims - 0.5).astype(int)\n","            label = np.zeros((coords.shape[0],),dtype=bool)\n","            select = np.logical_and(np.all(grid_coords>=0,axis=1), np.all(grid_coords<m1.dims,axis=1))\n","            label[select] = m1.data[grid_coords[select,0], grid_coords[select,1], grid_coords[select,2]]\n","\n","            gt_im = np.zeros(vis_orig.shape + (numDepth,), dtype=bool)   # [new_h, new_w, num_depth]\n","            trueOPlanes = torch.zeros(numDepth,1,800,1280)\n","            for ii in range(numDepth):\n","                gt_im[py_orig, px_orig, ii] = label[ii::numDepth]\n","                trueOPlanes[ii,0] = torch.tensor([1 if x else 0 for x in gt_im[:, :, ii].reshape(800*1280)]).reshape(800,1280)\n","#                 plt.imshow(trueOPlanes[ii,0])\n","#                 plt.show()\n","            \n","            return trueOPlanes, zVals\n","\n"],"metadata":{"id":"ZIrwSz-yfGV2","executionInfo":{"status":"ok","timestamp":1670107834653,"user_tz":360,"elapsed":2348,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!pip install trimesh"],"metadata":{"id":"MWfUMP1qNJXN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670106237519,"user_tz":360,"elapsed":4084,"user":{"displayName":"Jon Vincent Medenilla","userId":"17576446179490008004"}},"outputId":"2d4f95cb-28b8-4cfd-b1a1-fbf951841c1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: trimesh in /usr/local/lib/python3.8/dist-packages (3.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from trimesh) (1.21.6)\n"]}]},{"cell_type":"code","source":["# DATALOADER\n","import os\n","import numpy as np\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","from skimage import filters\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, csv_file, n_planes_for_train):\n","        # making dataframe \n","        self.df2 = pd.read_csv(csv_file) \n","        headerList = ['image', 'visible', 'depth', 'gt_oplane']\n","        self.df2.to_csv(\"inputs.csv\", header=headerList, index=False)\n","          \n","        df2 = pd.read_csv(\"inputs.csv\")\n","        #self.train = train\n","        #self.shuffle = shuffle\n","        self.n_planes_for_train = n_planes_for_train\n","        self.gt_oplanes = df2.loc[:,\"gt_oplane\"]\n","        self.img_dir = df2.loc[:,\"image\"]\n","        self.depth_dir = df2.loc[:,\"depth\"]\n","        self.visible_dir = df2.loc[:,\"visible\"]\n","        #self.transform = transform\n","        #self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_dir)\n","\n","    def grey_transform(self, np_array):\n","        #print(np_array.shape)\n","        grey_image = cv2.cvtColor(np_array, cv2.COLOR_BGR2GRAY)\n","        return grey_image\n","\n","    def concatenate_channels(self, img1, img2, axis=2):\n","        return np.concatenate((img1,img2), axis=2)\n","      \n","    def calculate_euclidean(self, np_array):\n","        return scipy.ndimage.distance_transform_edt(np_array)\n","\n","    def calculate_edges(self, grey_img):\n","        edges = filters.farid(grey_img)\n","        return edges\n","\n","    def resize_image(self, np_array, x_dim, y_dim):\n","        image_size = (x_dim, y_dim)\n","        image_resized = cv2.resize(np_array, image_size, interpolation=cv2.INTER_LINEAR)\n","        return image_resized\n","\n","    def add_channels(self, np_array):\n","        img_resized = self.resize_image(np_array, 512, 512)\n","\n","        grey_img = self.grey_transform(img_resized)\n","        edges = self.calculate_edges(grey_img)\n","        edges_resized = self.resize_image(edges, 512, 512)\n","        edges_resized = torch.from_numpy(edges_resized).unsqueeze(-1)\n","        edges_resized = edges_resized.cpu().detach().numpy()\n","\n","        euc_dist = self.calculate_euclidean(grey_img)\n","        euc_dist_resized = torch.from_numpy(euc_dist).unsqueeze(-1)\n","        euc_dist_resized = euc_dist_resized.cpu().detach().numpy()\n","\n","        concatenated_input = self.concatenate_channels(img_resized,euc_dist_resized, axis=2)\n","        concatenated_input = self.concatenate_channels(concatenated_input,edges_resized, axis=2)\n","\n","        return concatenated_input\n","\n","    def __getitem__(self, idx):\n","        #self.__len__\n","        #if self.shuffle:\n","        #  idx = random.randint(0,self.__len__)\n","\n","        #if self.train:\n","\n","\n","        image = cv2.imread(self.img_dir[idx])\n","        image = self.add_channels(image)\n","      \n","        label_obj = MeshObj(self.gt_oplanes[idx],max_depth_range,mesh_data_root,binvoxPathPrefix,extra_mesh_data_root)\n","        label, zVals = label_obj.getOPlanes(self.n_planes_for_train)\n","        # depth_range, max_z = label_obj.compute_visible_mesh_depth_range()\n","\n","        depth = np.load(self.depth_dir[idx])\n","        depth = self.resize_image(depth, 512,512)\n","        depth = np.expand_dims(depth, axis=0)\n","        depth = torch.from_numpy(depth)\n","\n","        visible = np.load(self.visible_dir[idx])\n","        #visible = self.resize_image(visible, 512,512)\n","        visible_low_res = self.resize_image(visible, 400, 640)\n","        visible = np.expand_dims(visible, axis=0)\n","        visible = torch.from_numpy(visible)\n","        visible_low_res = np.expand_dims(visible_low_res, axis=0)\n","        visible_low_res = torch.from_numpy(visible_low_res)\n","\n","        image = np.moveaxis(image, -1, 0)\n","        image = torch.from_numpy(image)\n","\n","        # label = np.moveaxis(label, -1, 0)\n","        # label = torch.from_numpy(label)\n","\n","        #print(torch.is_tensor(image), torch.is_tensor(label), torch.is_tensor(depth), torch.is_tensor(visible), torch.is_tensor(visible_low_res))\n","\n","        return image, label, depth, visible, visible_low_res, zVals\n","\n"],"metadata":{"id":"iCjX-G1fIXwQ","executionInfo":{"status":"ok","timestamp":1670107840610,"user_tz":360,"elapsed":450,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["'''FPN in PyTorch.\n","See the paper \"Feature Pyramid Networks for Object Detection\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import cv2\n","import time\n","from torch.autograd import Variable\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class FPN(nn.Module):\n","    def __init__(self, block, num_blocks):\n","        super(FPN, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3, bias=False) #THE FIRST NUM (3) IS THE NUMBER OF INPUT CHANNELS\n","        self.bn1 = nn.BatchNorm2d(64)\n","\n","        # Bottom-up layers\n","        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","\n","        # Top layer\n","        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n","\n","        # Smooth layers\n","        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","\n","        # Lateral layers\n","        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def _upsample_add(self, x, y):\n","        '''Upsample and add two feature maps.\n","        Args:\n","          x: (Variable) top feature map to be upsampled.\n","          y: (Variable) lateral feature map.\n","        Returns:\n","          (Variable) added feature map.\n","        Note in PyTorch, when input size is odd, the upsampled feature map\n","        with `F.upsample(..., scale_factor=2, mode='nearest')`\n","        maybe not equal to the lateral feature map size.\n","        e.g.\n","        original input size: [N,_,15,15] ->\n","        conv2d feature map size: [N,_,8,8] ->\n","        upsampled feature map size: [N,_,16,16]\n","        So we choose bilinear upsample which supports arbitrary output sizes.\n","        '''\n","        _,_,H,W = y.size()\n","        return F.upsample(x, size=(H,W), mode='bilinear') + y\n","\n","    def forward(self, x):\n","        # Bottom-up\n","        c1 = F.relu(self.bn1(self.conv1(x)))\n","        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n","        c2 = self.layer1(c1)\n","        c3 = self.layer2(c2)\n","        c4 = self.layer3(c3)\n","        c5 = self.layer4(c4)\n","        # Top-down\n","        p5 = self.toplayer(c5)\n","        p4 = self._upsample_add(p5, self.latlayer1(c4))\n","        p3 = self._upsample_add(p4, self.latlayer2(c3))\n","        p2 = self._upsample_add(p3, self.latlayer3(c2))\n","        # Smooth\n","        p4 = self.smooth1(p4)\n","        p3 = self.smooth2(p3)\n","        p2 = self.smooth3(p2)\n","        return p2\n","\n","\n","def FPN101():\n","    # return FPN(Bottleneck, [2,4,23,3])\n","    return FPN(Bottleneck, [2,2,2,2])\n","    \n","class FRGB(nn.Module):\n","    def __init__(self):\n","        super(FRGB, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,128,1)\n","        self.gn3 = nn.GroupNorm(32,128)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.gn3(self.conv3(x))\n","        return x\n","\n","class Fspatial(nn.Module):\n","    def __init__(self):\n","        super(Fspatial, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,1,1)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.conv3(x)\n","        return x\n","    \n","class TwoLayerCNN(nn.Module):\n","    def __init__(self):\n","        super(TwoLayerCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(64,128,1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","    \n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.gn2(self.conv2(x))\n","        return x\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        ####################\n","        num = (2*mask*op_true*op_pred).sum(dim = [2,3])\n","        d1 = (mask*op_true).sum(dim=[2,3])\n","        d2 = (mask*op_pred).sum(dim=[2,3])\n","        s = (num/(d1+d2)).sum()\n","        \n","        ####################\n","        #Should be equivalent to this:\n","#         for i in range(N):\n","#             num = 2*torch.sum(mask*op_true[i]*op_pred[i])\n","#             denom = torch.sum(mask*op_true[i])+torch.sum(mask*op_pred[i])\n","#             s = s + num/denom\n","    \n","        return s/N\n","    \n","\n","    \n","class BCELoss(nn.Module):\n","    def __init__(self):\n","        super(BCELoss,self).__init__()\n","    \n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        num = mask*((op_true*(torch.log(op_pred))) + (1-op_true)*torch.log(1-op_pred))                 \n","        s = num.sum()\n","        s = s/(N*mask.sum()) \n","    \n","        return s\n","        \n","        \n","        \n","\n","class ComboNet(nn.Module):\n","    def __init__(self, batchSize, numPeChannels, learningRate):\n","        super(ComboNet, self).__init__()\n","        self.f_FPN = FPN(Bottleneck, [3,4,6,3])\n","        self.f_RGB = FRGB()\n","        self.f_depth = TwoLayerCNN()\n","        self.f_spatial = Fspatial()\n","        \n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n","        self.lossBCE = BCELoss()\n","        self.lossDICE = DiceLoss()\n","        \n","        #Make denominator tensor for positional encoding (don't want to run duplicate work)\n","        denom = torch.zeros(batchSize,numPeChannels,256,256)\n","        idx = torch.ones(256,256)\n","        for i in range(numPeChannels):\n","            denom[:,i,:,:] = 200**(2*i*idx/numPeChannels)\n","        self.denom = denom\n","        self.numPeChannels = numPeChannels\n","        \n","        \n","    def forward(self,x1,x2,z):\n","        #print(x1.shape, x2.shape, z.shape)\n","        st = time.time()\n","        #RGB feature processing\n","        x1 = self.f_FPN(x1)\n","        x1_lowres = self.f_RGB(x1)\n","        x1 = F.interpolate(x1_lowres,scale_factor=2, mode='bilinear') #Upsampling step: convert 128x128 to 256x256 img\n","        \n","        #Depth feature processing\n","        x2 = self.positionEncoding(x2,z)\n","        x2_lowres = F.interpolate(x2,scale_factor=0.5, mode='bilinear') #Downsampled depth difference image\n","        x2 = self.f_depth(x2)\n","        x2_lowres = self.f_depth(x2_lowres)\n","        \n","        #Combine features and pass them through final CNN\n","        x = torch.cat((x1,x2), 1) #second arg specifies which dimension to concatenate on, we want channel dimension which is 1\n","        x = self.f_spatial(x)\n","        \n","        #Get low res OPlane for loss computation, use inner product (eqn 14 from paper)   \n","        multp  = x1_lowres * x2_lowres \n","        x_lowres = multp.sum(dim = 1, keepdim = True)\n","        \n","        #Normalize both outputs so all values are between 0 and 1\n","        x = x - x.min()\n","        x = x/x.max()\n","        x_lowres = x_lowres - x_lowres.min()\n","        x_lowres = x_lowres/x_lowres.max()\n","        \n","        return x, x_lowres\n","    \n","    def positionEncoding(self, depth, z):\n","        \"\"\"\n","        Computes the positional encoding (as defined by the paper) for a depth\n","        - depth: the input depth image\n","        - z: the distance we wish to evaluate\n","        \"\"\"\n","        print(\"pe shapes: \", depth.shape, z)\n","        depth = F.interpolate(depth,scale_factor=0.5, mode='bilinear')\n","        s = depth.size()\n","        pe = torch.zeros(s[0],self.numPeChannels,s[2],s[3])\n","        num = z-depth\n","        pe[:,0::2,:,:] = torch.sin(50*num/self.denom[:,0::2,:])\n","        pe[:,1::2,:,:] = torch.cos(50*num/self.denom[:,1::2,:])\n","        \n","        return pe\n","    \n","    def step(self,x_RGB,x_depth,mask,z_vals,op_true_highres):\n","        \"\"\"\n","        Iterates over a single training step, ie one image with a set of N values in the range [z_min, z_max]\n","        - x: input batch\n","        - y: expected labels for batch\n","        \"\"\"\n","        self.optimizer.zero_grad() #Reset parameter gradients to 0\n","        #Get the outputs for each values of z\n","        N = z_vals.size()[0]\n","        op_highres = torch.zeros(N,1,256,256)\n","        op_lowres = torch.zeros(N,1,128,128)\n","        \n","        st = time.time()\n","        for i, z in enumerate(z_vals):\n","            op_highres_i, op_lowres_i = self.forward(x_RGB,x_depth,z)\n","            #print(op_highres[i,:,:,:].shape, )\n","            op_highres[i,:,:,:] = op_highres_i\n","            op_lowres[i,:,:,:] = op_lowres_i\n","        print(\"Forward time: \",round(time.time()-st,2))\n","            \n","        \n","        #Calculate the loss based on the predicted OPlanes for all z values\n","        lambda_BCE, lambda_DICE = 1,1\n","\n","        # downsample to low resolution\n","        mask_lowres = F.interpolate(mask,scale_factor=0.5, mode='bilinear')\n","        # op_true_highres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","        op_true_lowres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","\n","        print(\"Highres sizes: \",op_true_highres.shape, op_highres.shape, mask.shape)\n","        print(\"Lowres sizes: \",op_true_lowres.shape, op_lowres.shape, mask_lowres.shape)\n","\n","        # # upscale everything to originial dimension (same as GT O-Plane)\n","        # op_true_highres = F.interpolate(op_true_highres,(800,1280), mode='bilinear')\n","        # op_highres = F.interpolate(op_highres,(800,1280), mode='bilinear')\n","        \n","        loss_highres = lambda_BCE*self.lossBCE(op_true_highres, op_highres, mask, N) + lambda_DICE*self.lossDICE(op_true_highres, op_highres, mask, N)\n","        loss_lowres = lambda_BCE*self.lossBCE(op_true_lowres, op_lowres, mask_lowres, N) + lambda_DICE*self.lossDICE(op_true_lowres, op_lowres, mask_lowres, N)    \n","\n","        loss = loss_highres + loss_lowres\n","        st = time.time()\n","        #loss.requires_grad = True\n","        loss.backward()\n","        print(\"Backward time: \",round(time.time()-st,2))\n","        \n","        self.optimizer.step()\n","\n","        return loss.detach().cpu().numpy()"],"metadata":{"id":"_rZjKbWHwLFg","executionInfo":{"status":"ok","timestamp":1670107845266,"user_tz":360,"elapsed":411,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.sampler import SubsetRandomSampler\n","import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from pathlib import Path\n","import torch\n","import os.path\n","import cv2\n","import scipy.ndimage\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","#from modules import ComboNet\n","\n","path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","batch_size = 1\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","#print(dataset_size)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","#print(train_indices, val_indices)\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n","                                           sampler=train_sampler, persistent_workers=True, num_workers=2)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler,persistent_workers=True, num_workers=2)\n","\n","N = 10\n","#print(train_loader[1])\n","net = ComboNet(1,64,0.001)\n","# Usage Example:\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    print(\"Processing epoch \",epoch)\n","    # Train:   \n","    for i, data in enumerate(train_loader):\n","        #print(data[0].shape)\n","        # z_min = data[6] #Calculated using min(depth OR mask), ie the point on the object of interest that is closest to the camera\n","        # z_range = data[5] #For training we need to get this from the the mesh to find the ground truth range of depths\n","        # z_vals = z_range*torch.rand(N) + z_min #create 10 random values between z_min and z_max\n","        \n","        op_truth = data[1].to(dtype=torch.float32) \n","        mask = data[3].to(dtype=torch.float32)\n","        mask_lowres = data[4].to(dtype=torch.float32)\n","        z_vals = data[4].to(dtype=torch.float32)\n","        \n","        \n","        #Get RGB and Depth images from data\n","        ipt1 = data[0].to(dtype=torch.float32) # image\n","        ipt2 = data[2].to(dtype=torch.float32) # depth\n","        #print(op_truth.shape)\n","        print(\"Stepping for data\")\n","        net.step(ipt1,ipt2,mask,z_vals,op_truth)\n","\n","\n"],"metadata":{"id":"QVSVg1IAaPXJ","colab":{"base_uri":"https://localhost:8080/","height":621},"executionInfo":{"status":"error","timestamp":1670107871480,"user_tz":360,"elapsed":4588,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"eafe5474-5323-40e5-af5e-64d1cebf02f1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing epoch  0\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-aba81264adf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing epoch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Train:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(data[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# z_min = data[6] #Calculated using min(depth OR mask), ie the point on the object of interest that is closest to the camera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-19-ed4617fb7987>\", line 86, in __getitem__\n    label_obj = MeshObj(self.gt_oplanes[idx],max_depth_range,mesh_data_root,binvoxPathPrefix,extra_mesh_data_root)\n  File \"<ipython-input-18-a2517fe35cb7>\", line 56, in __init__\n    assert os.path.exists(self.binvox), f\"{self.binvox}\"\nAssertionError: demodata/sol_3_int/0004_Ped_000000788622594_000000000481033_00/000160_mesh/voxel_256.binvox2\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import torch\n","x = torch.ones(1,256,256)\n","y = torch.zeros(4, 1,256,256)\n","for i in range(4):\n","  y[i,:,:,:] = x\n","\n","print(y.shape)\n","x = torch.unsqueeze(x, dim=0)\n","print(x.dtype)\n","\n","z = F.interpolate(y,(800,1280), mode='bilinear')\n","print(z.shape)"],"metadata":{"id":"npWQLo_h5TvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_obj = MeshObj(self.gt_oplanes[idx],max_depth_range,mesh_data_root,binvoxPathPrefix,extra_mesh_data_root)\n","label = label_obj.Get_GroundTruth(n_planes_for_train)"],"metadata":{"id":"Vv-8j_CufC0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pixel_mean = np.array([123.675, 116.280, 103.530]).reshape((1, 1, 3))\n","pixel_std = np.array([58.395, 57.120, 57.375]).reshape((1, 1, 3))\n","max_depth_range = 2.1\n","n_planes_for_train = 5\n","n_planes_for_val = 20\n","n_bins_for_plane_hrchy_sampling = 20\n","mesh_data_root = 'demodata'\n","data_h = 512\n","data_w = 512\n","crop_expand_ratio = 0.1\n","binvoxPathPrefix = 'demodata'\n","use_adaptive_sampling = False\n","bin_sample_replace = False\n","depth_range_expand_ratio = 0.1\n","use_masked_out_img = False\n","extra_mesh_data_root = None"],"metadata":{"id":"3kEmEO_wft0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from dataloader import CustomImageDataset\n","path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","dataset = CustomImageDataset(csv_file=path + '/' + 'inputs.csv', n_planes_for_train=5)\n","\n","for i in range(1):\n","  print(dataset[i][4].shape)"],"metadata":{"id":"C--7Khd3UR4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainModel(N):\n","    torch.autograd.set_detect_anomaly(False)\n","    \n","    #Instantiate model\n","    net = ComboNet(1,64,0.001)\n","    \n","    #Get data \n","    epochs = 1\n","    data = torch.ones(1,3,512,512)\n","    \n","    #Training loop\n","    st = time.time()\n","    for i in range(epochs):\n","        print(\"Processing epoch \",i)\n","        \n","        for d in data:\n","            \n","            #Generate set of Z's\n","            z_min = 1 #Calculated using min(depth OR mask), ie the point on the object of interest that is closest to the camera\n","            z_range = 2 #For training we need to get this from the the mesh to find the ground truth range of depths\n","            z_vals = z_range*torch.rand(N) + z_min #create 10 random values between z_min and z_max\n","            \n","            op_truth = torch.ones(N,1,512,512)#Q: WHERE TO GET THESE FROM? THE MESH? Dimension is N samples, each with one channel and size HxW\n","            mask = torch.ones(1,1,512,512)\n","            mask_lowres = torch.ones(1,1,128,128)\n","            \n","            \n","            #Get RGB and Depth images from data\n","            ipt1 = torch.ones(1,3,512,512)\n","            ipt2 = torch.ones(1,1,512,512)\n","            net.step(ipt1,ipt2,mask,z_vals,op_truth)\n","            \n","\n","\n","    print(\"Total training time: \",round(time.time()-st,2))\n","    \n","\n","trainModel(2) #N (number of OPlanes we sample per image) is 10 during training"],"metadata":{"id":"9QFvDdqZc-M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/ECE 544 Project/VincentsWorkspace'\n","print(path + '/' + 'inputs.csv')"],"metadata":{"id":"aI4up2W_VydN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"vz88vhHITVr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display image and label.\n","import matplotlib.pyplot as plt\n","\n","train_features, train_labels = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0].squeeze()\n","label = train_labels[0]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"],"metadata":{"id":"h8P_p5S5IdoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas module \n","import pandas as pd \n","    \n","# making dataframe \n","df = pd.read_csv(\"inputs.csv\") \n","headerList = ['image', 'visible', 'depth', 'gt_oplane']\n","df.to_csv(\"inputs.csv\", header=headerList, index=False)\n","   \n","df2 = pd.read_csv(\"inputs.csv\")\n","print(df2['gt_oplane'][0])\n"],"metadata":{"id":"gS7hZvHMMI60"},"execution_count":null,"outputs":[]}]}