{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3sw_jraJ8TAz"},"outputs":[],"source":["'''FPN in PyTorch.\n","See the paper \"Feature Pyramid Networks for Object Detection\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Res_block(nn.Module):\n","    def __init__(self, in_channels, out_channels,sample):\n","        super(Res_block, self).__init__()\n","        self.in_planes = 64\n","\n","        if sample:\n","          self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n","          self.parallel = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n","                nn.BatchNorm2d(out_channels)\n","\n","        else:\n","          self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","          self.parallel = nn.Sequential()\n","\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","\n","        def forward(self, input):\n","          parallel = self.parallel(input)\n","          input = nn.ReLU(\n","                      (self.bn2(\n","                            self.conv2((\n","                              nn.ReLU(\n","                                  (self.bn1(self.conv1(input)))\n","                                     ))))))\n","          input = input + parallel\n","          return nn.ReLU(input))\n","\n","class FPN(nn.Module):\n","  def __init__(self, in_channels, resblock, outputs=256):\n","    super().__init__()\n","        self.layer0 = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","\n","        self.layer1 = nn.Sequential(\n","            resblock(64, 64, sample=False),\n","            resblock(64, 64, sample=False)\n","        )\n","\n","        self.layer2 = nn.Sequential(\n","            resblock(64, 128, sample=True),\n","            resblock(128, 128, sample=False)\n","        )\n","\n","        self.layer3 = nn.Sequential(\n","            resblock(128, 256, sample=True),\n","            resblock(256, 256, sample=False)\n","        )\n","\n","\n","        self.layer4 = nn.Sequential(\n","            resblock(256, 512, sample=True),\n","            resblock(512, 512, sample=False)\n","        )\n","\n","        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n","        self.fc = torch.nn.Linear(512, outputs)\n","\n","        # # Bottom-up layers\n","        # self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n","        # self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        # self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        # self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n","        # self.layer5 = self._make_layer(block, 256, num_blocks[4], stride=2)\n","        # self.layer6 = self._make_layer(block, 256, num_blocks[5], stride=2)\n","        # self.layer7 = self._make_layer(block, 512, num_blocks[6], stride=2)\n","        # self.layer8 = self._make_layer(block, 512, num_blocks[7], stride=2)\n","        # self.layer9 = self._make_layer(block, 512, num_blocks[8]], stride=2)\n","        # self.layer10 = self._make_layer(block, 512, num_blocks[9], stride=2)\n","\n","        # # Top layer\n","        # self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n","\n","        # # Smooth layers\n","        # self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        # self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        # self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","\n","        # # Lateral layers\n","        # self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n","        # self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n","        # self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n","\n","    # def _make_layer(self, block, planes, num_blocks, stride):\n","    #     strides = [stride] + [1]*(num_blocks-1)\n","    #     layers = []\n","    #     for stride in strides:\n","    #         layers.append(block(self.in_planes, planes, stride))\n","    #         self.in_planes = planes * block.expansion\n","    #     return nn.Sequential(*layers)\n","\n","    # def forward(self, x):\n","    #     # Bottom-up\n","    #     c1 = F.relu(self.bn1(self.conv1(x)))\n","    #     c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n","    #     c5 = self.layer4(self.layer3(self.layer2(self.layer1(c1))))\n","       \n","    #     # Top-down\n","    #     p5 = self.toplayer(c5)\n","    #     p4 = self._upsample_add(p5, self.latlayer1(c4))\n","    #     p3 = self._upsample_add(p4, self.latlayer2(c3))\n","    #     p2 = self._upsample_add(p3, self.latlayer3(c2))\n","    #     # Smooth\n","    #     p4 = self.smooth1(p4)\n","    #     p3 = self.smooth2(p3)\n","    #     p2 = self.smooth3(p2)\n","    #     return p2\n","\n","    def forward(self, input):\n","        input = self.layer0(input)\n","        input = self.layer1(input)\n","        input = self.layer2(input)\n","        input = self.layer3(input)\n","        input = self.layer4(input)\n","        input = self.gap(input)\n","        input = torch.flatten(input)\n","        input = self.fc(input)\n","\n","        return input\n","\n","    def _upsample_add(self, x, y):\n","        '''Upsample and add two feature maps.\n","        Args:\n","          x: (Variable) top feature map to be upsampled.\n","          y: (Variable) lateral feature map.\n","        Returns:\n","          (Variable) added feature map.\n","        Note in PyTorch, when input size is odd, the upsampled feature map\n","        with `F.upsample(..., scale_factor=2, mode='nearest')`\n","        maybe not equal to the lateral feature map size.\n","        e.g.\n","        original input size: [N,_,15,15] ->\n","        conv2d feature map size: [N,_,8,8] ->\n","        upsampled feature map size: [N,_,16,16]\n","        So we choose bilinear upsample which supports arbitrary output sizes.\n","        '''\n","        _,_,H,W = y.size()\n","        return F.upsample(x, size=(H,W), mode='bilinear') + y\n","\n","    \n","\n","\n","def FPN101():\n","    # return FPN(Bottleneck, [2,4,23,3])\n","    return FPN(Bottleneck, [2,2,2,2])\n","\n","\n","def test():\n","    net = FPN101()\n","    fm = net(torch.randn(1,3,128,128))\n","\n","test()"]},{"cell_type":"code","source":["class FRGB(nn.Module):\n","    def __init__(self):\n","        super(FRGB, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,128,1)\n","        self.gn3 = nn.GroupNorm(32,128)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.gn3(self.conv3(x))\n","        return x\n","\n","class Fspatial(nn.Module):\n","    def __init__(self):\n","        super(Fspatial, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,1,1)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.conv3(x)\n","        return x\n","    \n","class TwoLayerCNN(nn.Module):\n","    def __init__(self):\n","        super(TwoLayerCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(64,128,1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","    \n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.gn2(self.conv2(x))\n","        return x\n"],"metadata":{"id":"_zA-Wj8q8ZR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","#PyTorch\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = F.sigmoid(inputs)       \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        \n","        return 1 - dice\n","    \n","\n","class ComboNet(nn.Module):\n","    def __init__(self, batchSize, numPeChannels, learningRate):\n","        super(ComboNet, self).__init__()\n","        self.f_FPN = FPN(Bottleneck, [2,2,2,2])\n","        self.f_RGB = FRGB()\n","        self.f_depth = TwoLayerCNN()\n","        self.f_spatial = Fspatial()\n","        \n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n","        self.lossBCE = nn.BCELoss()\n","        self.lossDICE = DiceLoss()\n","        self.lossCE = nn.CrossEntropyLoss() #THIS IS JUST HERE AS A DUMMY FOR NOW\n","        \n","        #Make denominator tensor for positional encoding (don't want to run duplicate work)\n","        denom = torch.zeros(batchSize,numPeChannels,256,256)\n","        idx = torch.ones(256,256)\n","        for i in range(numPeChannels):\n","            denom[:,i,:,:] = 200**(2*i*idx/numPeChannels)\n","        self.denom = denom\n","        self.numPeChannels = numPeChannels\n","        \n","        \n","    def forward(self,x1,x2,z):\n","        #RGB feature processing\n","        x1 = self.f_FPN(x1)\n","        x1 = self.f_RGB(x1)\n","        x1 = torch.nn.functional.interpolate(x1,scale_factor=2, mode='bilinear') #Convert 128x128 to 256x256 img\n","        \n","        #Depth feature processing\n","        x2 = self.positionEncoding(x2,z)\n","        x2 = self.f_depth(x2)\n","        \n","        #Combine features and pass them through final CNN\n","        x = torch.cat((x1,x2), 1) #second arg specifies which dimension to concatenate on, we want channel dimension which is 1\n","        x = self.f_spatial(x)\n","        \n","        return x\n","    \n","    def positionEncoding(self, depth, z):\n","        \"\"\"\n","        Computes the positional encoding (as defined by the paper) for a depth\n","        - depth: the input depth image\n","        - z: the distance we wish to evaluate\n","        \"\"\"\n","        \n","        depth = torch.nn.functional.interpolate(depth,scale_factor=0.5, mode='bilinear')\n","        s = depth.size()\n","        pe = torch.zeros(s[0],self.numPeChannels,s[2],s[3])\n","        pe[:,0::2,:,:] = torch.sin((50*depth)/self.denom[:,0::2,:])\n","        pe[:,1::2,:,:] = torch.cos((50*depth)/self.denom[:,1::2,:])\n","        \n","        return pe\n","    \n","    def step(self,x1,x2,z,y):\n","        \"\"\"\n","        Iterates over a single training step\n","        - x: input batch\n","        - y: expected labels for batch\n","        \"\"\"\n","        self.optimizer.zero_grad() #Reset parameter gradients to 0\n","\n","        outputs = self.forward(x1,x2,z)\n","#         loss = self.lossBCE(outputs,y) + self.lossDICE(outputs,y)\n","        loss = self.lossCE(outputs,y) + self.lossCE(outputs,y) #dummy example showing we can easily sum losses\n","        loss.backward()\n","        self.optimizer.step()\n","        \n","        return loss.detach().cpu().numpy()\n","        \n","        \n","        \n","        \n","\n","    \n","def trainModel():\n","    \n","    #Instantiate model\n","    net = ComboNet(1,64,0.001)\n","    \n","    #Get data \n","    epochs = 10\n","    data = torch.randn(1,3,512,512)\n","    \n","    #Training loop\n","    st = time.time()\n","    for i in range(epochs):\n","        print(\"Processing epoch \",i)\n","        \n","        for d in data:\n","            ipt1 = torch.randn(1,3,512,512)\n","            ipt2 = torch.ones(1,1,512,512)\n","            y = torch.randn(1,1,256,256)\n","            net.step(ipt1,ipt2,1,y)\n","        \n","    print(\"Total training time: \",round(time.time()-st,2))\n","    \n","    \n","\n","trainModel()\n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLawAA9g8n-v","executionInfo":{"status":"ok","timestamp":1667354135230,"user_tz":300,"elapsed":86849,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"e6b30247-c836-4fa0-ae4e-9a63d1fac44a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing epoch  0\n","Processing epoch  1\n","Processing epoch  2\n","Processing epoch  3\n","Processing epoch  4\n","Processing epoch  5\n","Processing epoch  6\n","Processing epoch  7\n","Processing epoch  8\n","Processing epoch  9\n","Total training time:  86.2\n"]}]}]}