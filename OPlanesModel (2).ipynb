{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3sw_jraJ8TAz","executionInfo":{"status":"ok","timestamp":1669749916193,"user_tz":360,"elapsed":2814,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"298dc158-794c-482b-907d-2710cace1ea0","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3722: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"]}],"source":["'''FPN in PyTorch.\n","See the paper \"Feature Pyramid Networks for Object Detection\" for more details.\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class FPN(nn.Module):\n","    def __init__(self, block, num_blocks):\n","        super(FPN, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3, bias=False) #THE FIRST NUM (3) IS THE NUMBER OF INPUT CHANNELS\n","        self.bn1 = nn.BatchNorm2d(64)\n","\n","        # Bottom-up layers\n","        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","\n","        # Top layer\n","        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n","\n","        # Smooth layers\n","        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","\n","        # Lateral layers\n","        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n","        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def _upsample_add(self, x, y):\n","        '''Upsample and add two feature maps.\n","        Args:\n","          x: (Variable) top feature map to be upsampled.\n","          y: (Variable) lateral feature map.\n","        Returns:\n","          (Variable) added feature map.\n","        Note in PyTorch, when input size is odd, the upsampled feature map\n","        with `F.upsample(..., scale_factor=2, mode='nearest')`\n","        maybe not equal to the lateral feature map size.\n","        e.g.\n","        original input size: [N,_,15,15] ->\n","        conv2d feature map size: [N,_,8,8] ->\n","        upsampled feature map size: [N,_,16,16]\n","        So we choose bilinear upsample which supports arbitrary output sizes.\n","        '''\n","        _,_,H,W = y.size()\n","        return F.upsample(x, size=(H,W), mode='bilinear') + y\n","\n","    def forward(self, x):\n","        # Bottom-up\n","        c1 = F.relu(self.bn1(self.conv1(x)))\n","        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n","        c2 = self.layer1(c1)\n","        c3 = self.layer2(c2)\n","        c4 = self.layer3(c3)\n","        c5 = self.layer4(c4)\n","        # Top-down\n","        p5 = self.toplayer(c5)\n","        p4 = self._upsample_add(p5, self.latlayer1(c4))\n","        p3 = self._upsample_add(p4, self.latlayer2(c3))\n","        p2 = self._upsample_add(p3, self.latlayer3(c2))\n","        # Smooth\n","        p4 = self.smooth1(p4)\n","        p3 = self.smooth2(p3)\n","        p2 = self.smooth3(p2)\n","        return p2\n","\n","\n","def FPN101():\n","    # return FPN(Bottleneck, [2,4,23,3])\n","    return FPN(Bottleneck, [2,2,2,2])\n","\n","\n","def test():\n","    net = FPN101()\n","    fm = net(torch.randn(1,3,128,128))\n","\n","test()"]},{"cell_type":"code","source":["class FRGB(nn.Module):\n","    def __init__(self):\n","        super(FRGB, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,128,1)\n","        self.gn3 = nn.GroupNorm(32,128)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.gn3(self.conv3(x))\n","        return x\n","\n","class Fspatial(nn.Module):\n","    def __init__(self):\n","        super(Fspatial, self).__init__()\n","        self.conv1 = nn.Conv2d(256,128,3,padding=1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,3,padding=1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","        self.ReLU2 = nn.ReLU()\n","        self.conv3 = nn.Conv2d(128,1,1)\n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.ReLU2(self.gn2(self.conv2(x)))\n","        x = self.conv3(x)\n","        return x\n","    \n","class TwoLayerCNN(nn.Module):\n","    def __init__(self):\n","        super(TwoLayerCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(64,128,1)\n","        self.gn1 = nn.GroupNorm(32,128)\n","        self.ReLU1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(128,128,1)\n","        self.gn2 = nn.GroupNorm(32,128)\n","    \n","    \n","    def forward(self,x):\n","        x = self.ReLU1(self.gn1(self.conv1(x)))\n","        x = self.gn2(self.conv2(x))\n","        return x\n"],"metadata":{"id":"_zA-Wj8q8ZR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","    \n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        ####################\n","        num = (2*mask*op_true*op_pred).sum(dim = [2,3])\n","        d1 = (mask*op_true).sum(dim=[2,3])\n","        d2 = (mask*op_pred).sum(dim=[2,3])\n","        s = (num/(d1+d2)).sum()\n","        \n","        ####################\n","        #Should be equivalent to this:\n","#         for i in range(N):\n","#             num = 2*torch.sum(mask*op_true[i]*op_pred[i])\n","#             denom = torch.sum(mask*op_true[i])+torch.sum(mask*op_pred[i])\n","#             s = s + num/denom\n","    \n","        return s/N\n","    \n","\n","    \n","class BCELoss(nn.Module):\n","    def __init__(self):\n","        super(BCELoss,self).__init__()\n","    \n","    def forward(self, op_true, op_pred, mask, N):\n","        \n","        num = mask*((op_true*(torch.log(op_pred))) + (1-op_true)*torch.log(1-op_pred))                 \n","        s = num.sum()\n","        s = s/(N*mask.sum()) \n","    \n","        return s\n","        \n","        \n","        \n","\n","class ComboNet(nn.Module):\n","    def __init__(self, batchSize, numPeChannels, learningRate):\n","        super(ComboNet, self).__init__()\n","        self.f_FPN = FPN(Bottleneck, [3,4,6,3])\n","        self.f_RGB = FRGB()\n","        self.f_depth = TwoLayerCNN()\n","        self.f_spatial = Fspatial()\n","        \n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n","        self.lossBCE = BCELoss()\n","        self.lossDICE = DiceLoss()\n","        \n","        #Make denominator tensor for positional encoding (don't want to run duplicate work)\n","        denom = torch.zeros(batchSize,numPeChannels,256,256)\n","        idx = torch.ones(256,256)\n","        for i in range(numPeChannels):\n","            denom[:,i,:,:] = 200**(2*i*idx/numPeChannels)\n","        self.denom = denom\n","        self.numPeChannels = numPeChannels\n","        \n","        \n","    def forward(self,x1,x2,z):\n","        st = time.time()\n","        #RGB feature processing\n","        x1 = self.f_FPN(x1)\n","        x1_lowres = self.f_RGB(x1)\n","        x1 = F.interpolate(x1_lowres,scale_factor=2, mode='bilinear') #Upsampling step: convert 128x128 to 256x256 img\n","        \n","        #Depth feature processing\n","        x2 = self.positionEncoding(x2,z)\n","        x2_lowres = F.interpolate(x2,scale_factor=0.5, mode='bilinear') #Downsampled depth difference image\n","        x2 = self.f_depth(x2)\n","        x2_lowres = self.f_depth(x2_lowres)\n","        \n","        #Combine features and pass them through final CNN\n","        x = torch.cat((x1,x2), 1) #second arg specifies which dimension to concatenate on, we want channel dimension which is 1\n","        x = self.f_spatial(x)\n","        \n","        #Get low res OPlane for loss computation, use inner product (eqn 14 from paper)   \n","        multp  = x1_lowres * x2_lowres \n","        x_lowres = multp.sum(dim = 1, keepdim = True)\n","        \n","        #Normalize both outputs so all values are between 0 and 1\n","        x = x - x.min()\n","        x = x/x.max()\n","        x_lowres = x_lowres - x_lowres.min()\n","        x_lowres = x_lowres/x_lowres.max()\n","        \n","        return x, x_lowres\n","    \n","    def positionEncoding(self, depth, z):\n","        \"\"\"\n","        Computes the positional encoding (as defined by the paper) for a depth\n","        - depth: the input depth image\n","        - z: the distance we wish to evaluate\n","        \"\"\"\n","        \n","        depth = F.interpolate(depth,scale_factor=0.5, mode='bilinear')\n","        s = depth.size()\n","        pe = torch.zeros(s[0],self.numPeChannels,s[2],s[3])\n","        num = z-depth\n","        pe[:,0::2,:,:] = torch.sin(50*num/self.denom[:,0::2,:])\n","        pe[:,1::2,:,:] = torch.cos(50*num/self.denom[:,1::2,:])\n","        \n","        return pe\n","    \n","    def step(self,x_RGB,x_depth,mask,z_vals,op_true_highres):\n","        \"\"\"\n","        Iterates over a single training step, ie one image with a set of N values in the range [z_min, z_max]\n","        - x: input batch\n","        - y: expected labels for batch\n","        \"\"\"\n","        self.optimizer.zero_grad() #Reset parameter gradients to 0\n","        \n","        #Get the outputs for each values of z\n","        N = z_vals.size()[0]\n","        op_highres = torch.zeros(N,1,256,256)\n","        op_lowres = torch.zeros(N,1,128,128)\n","        st = time.time()\n","        for i, z in enumerate(z_vals):\n","            op_highres_i, op_lowres_i = self.forward(x_RGB,x_depth,z)\n","            op_highres[i,:,:,:] = op_highres_i\n","            op_lowres[i,:,:,:] = op_lowres_i\n","        print(\"Forward time: \",round(time.time()-st,2))\n","            \n","        \n","        #Calculate the loss based on the predicted OPlanes for all z values\n","        lambda_BCE, lambda_DICE = 1,1\n","        mask_lowres = F.interpolate(mask,scale_factor=0.25, mode='bilinear')\n","        mask = F.interpolate(mask,scale_factor=0.5, mode='bilinear')\n","        op_true_highres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","        op_true_lowres = F.interpolate(op_true_highres,scale_factor=0.5, mode='bilinear')\n","        \n","        loss_highres = lambda_BCE*self.lossBCE(op_true_highres, op_highres, mask, N) + lambda_DICE*self.lossDICE(op_true_highres, op_highres, mask, N)\n","        loss_lowres = lambda_BCE*self.lossBCE(op_true_lowres, op_lowres, mask_lowres, N) + lambda_DICE*self.lossDICE(op_true_lowres, op_lowres, mask_lowres, N)    \n","\n","        loss = loss_highres + loss_lowres\n","        st = time.time()\n","        loss.backward()\n","        print(\"Backward time: \",round(time.time()-st,2))\n","        \n","        self.optimizer.step()\n","\n","        return loss.detach().cpu().numpy()\n","        \n","        \n","        \n","        \n","\n","    \n","def trainModel(N):\n","    torch.autograd.set_detect_anomaly(False)\n","    \n","    #Instantiate model\n","    net = ComboNet(1,64,0.001)\n","    \n","    #Get data \n","    epochs = 1\n","    data = torch.ones(1,3,512,512)\n","    \n","    #Training loop\n","    st = time.time()\n","    for i in range(epochs):\n","        print(\"Processing epoch \",i)\n","        \n","        for d in data:\n","            \n","            #Generate set of Z's\n","            z_min = 1 #Calculated using min(depth OR mask), ie the point on the object of interest that is closest to the camera\n","            z_range = 2 #For training we need to get this from the the mesh to find the ground truth range of depths\n","            z_vals = z_range*torch.rand(N) + z_min #create 10 random values between z_min and z_max\n","            \n","            op_truth = torch.ones(N,1,512,512)#Q: WHERE TO GET THESE FROM? THE MESH? Dimension is N samples, each with one channel and size HxW\n","            mask = torch.ones(1,1,512,512)\n","            mask_lowres = torch.ones(1,1,128,128)\n","            \n","            \n","            #Get RGB and Depth images from data\n","            ipt1 = torch.ones(1,3,512,512)\n","            ipt2 = torch.ones(1,1,512,512)\n","            net.step(ipt1,ipt2,mask,z_vals,op_truth)\n","            \n","\n","\n","    print(\"Total training time: \",round(time.time()-st,2))\n","    \n","\n","trainModel(2) #N (number of OPlanes we sample per image) is 10 during training\n","\n","\n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"uLawAA9g8n-v","executionInfo":{"status":"error","timestamp":1670024498632,"user_tz":360,"elapsed":6,"user":{"displayName":"Viktor Ladics","userId":"15628888991227696432"}},"outputId":"66773c7e-b287-4655-90c2-987d8038c9c7"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7621003b04bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDiceLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]}]}